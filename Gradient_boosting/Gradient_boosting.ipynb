{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "plt.style.use('ggplot')\n",
    "#from feature_engineering import add_retning\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('../data')\n",
    "\n",
    "from display_data import import_data\n",
    "\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_value(df):\n",
    "    #layout, set everythin to 1\n",
    "    df['layout'].fillna(int(1.0),inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    #found correleation between number of stories and balconies,loggias\n",
    "    df.loc[(df['stories'] < 7.0)&df['loggias'].isna(),'loggias'] = 0\n",
    "    df.loc[(df['stories'] < 7.0)&df['balconies'].isna(),'balconies'] = 1\n",
    "\n",
    "    df.loc[((df['stories'] > 12) & (df['stories'] < 16))|((df['stories'] > 17) & (df['stories'] < 24))&(df['loggias'].isna()),'loggias'] = 1\n",
    "    df.loc[((df['stories'] > 12) & (df['stories'] < 16))|((df['stories'] > 17) & (df['stories'] < 24))&(df['loggias'].isna()),'balconies'] = 0\n",
    "\n",
    "\n",
    "    #use the correleation between balconies and loggias to replace nans,\n",
    "    df.loc[(df['loggias'] == 0.0 )& df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[(df['balconies'] == 0.0 )& df['loggias'].isna(),'loggias'] = 1\n",
    "    df.loc[(df['loggias'] > 0.0 )& df['balconies'].isna(),'balconies'] = 0\n",
    "    df.loc[(df['balconies'] > 0.0 )& df['loggias'].isna(),'loggias'] = 0\n",
    "\n",
    "    #use correlation between material and ceiling groups\n",
    "    ceiling_under_2 = len(df[((df['material'] == 4)|(df['material'] == 3)) & df['ceiling'].isna()])\n",
    "    sample = df[df['ceiling'] < 2.6]['ceiling'].sample(n=ceiling_under_2,replace=True).to_numpy()\n",
    "    df.loc[((df['material'] == 4)|(df['material'] == 3)) & (df['ceiling'].isna()),'ceiling'] = sample\n",
    "    #use corelation between condition and ceiling groups\n",
    "    ceiling_under_2 = len(df[(df['condition'] == 1) & df['ceiling'].isna()])\n",
    "    sample = df[df['ceiling'] < 2.6]['ceiling'].sample(n=ceiling_under_2,replace=True).to_numpy()\n",
    "    df.loc[(df['condition'] == 1) & df['ceiling'].isna(),'ceiling'] = sample\n",
    "\n",
    "\n",
    "    #use correleation betwwen ceiling and area\n",
    "    ceiling_over_3 = len(df[(df['area_total'] > 104)&(df['ceiling'].isna())])\n",
    "    sample = df[df['ceiling'] > 3.0]['ceiling'].sample(n=ceiling_over_3,replace=True).to_numpy()\n",
    "    df.loc[(df['area_total'] > 104)&(df['ceiling'].isna() ),'ceiling'] = sample\n",
    "\n",
    "    ceiling_over_3 = len(df[(df['area_total'] > 75)&(df['ceiling'].isna())])\n",
    "    sample = df[df['ceiling'] > 2.95]['ceiling'].sample(n=ceiling_over_3,replace=True).to_numpy()\n",
    "    df.loc[(df['area_total'] > 75)&(df['ceiling'].isna() ),'ceiling'] = sample\n",
    "\n",
    "    ceiling_under_2 = len(df[(df['area_total'] < 55)&(df['ceiling'].isna())])\n",
    "    sample = df[df['ceiling'] < 2.8]['ceiling'].sample(n=ceiling_under_2,replace=True).to_numpy()\n",
    "    df.loc[(df['area_total'] < 55)&(df['ceiling'].isna() ),'ceiling'] = sample\n",
    "\n",
    "    #seller\n",
    "    #use correleation between seller and new\n",
    "    df.loc[(df['new'] == 1 )& df['seller'].isna(),'seller'] = 3\n",
    "\n",
    "    #use correleation between seller and constructed\n",
    "    df.loc[(df['constructed'] > 2018)& df['seller'].isna(),'seller'] = 3\n",
    "    df.loc[(df['constructed'] < 1990)& df['seller'].isna(),'seller'] = 1\n",
    "\n",
    "    #use correleation between seller and stories\n",
    "    df.loc[(df['stories'] > 17)& df['seller'].isna(),'seller'] = 3\n",
    "    df.loc[(df['stories'] < 9)& df['seller'].isna(),'seller'] = 1\n",
    "\n",
    "    #use correleation between seller and district\n",
    "    df.loc[((df['district'] == 3)|(df['district']==6))& df['seller'].isna(),'seller'] = 1\n",
    "    df.loc[((df['district'] == 11)|(df['district']==2))& df['seller'].isna(),'seller'] = 3\n",
    "\n",
    "    #conditon\n",
    "    #use correleation between condition and new\n",
    "    df.loc[(df['new'] == 1)& df['condition'].isna(),'condition'] = 0\n",
    "\n",
    "    #use correleation between condition and constructed year\n",
    "    df.loc[(df['constructed'] >= 2017)& df['condition'].isna(),'condition'] = 0\n",
    "    df.loc[(df['constructed'] <= 1989)& df['condition'].isna(),'condition'] = 1\n",
    "\n",
    "    #material\n",
    "    #use correlation between material and district\n",
    "    df.loc[(df['district'] == 3)& df['material'].isna(),'material'] = 3\n",
    "\n",
    "    #use correleation between material and constructed year\n",
    "    df.loc[((df['constructed'] > 1970)& (df['constructed'] <= 1989))&df['material'].isna(),'material'] = 3\n",
    "    df.loc[(df['constructed'] <= 1970)&df['material'].isna(),'material'] = 0\n",
    "\n",
    "    #use correleation between stories and material\n",
    "    df.loc[(df['stories'] < 7)& df['material'].isna(),'material'] = 0\n",
    "    df.loc[((df['stories'] > 7)&(df['stories'] < 9))& df['material'].isna(),'material'] = 3\n",
    "\n",
    "    #use correleation between conditon and material\n",
    "    df.loc[(df['condition'] == 3)& df['material'].isna(),'material'] = 3\n",
    "    df.loc[(df['condition'] == 0)& df['material'].isna(),'material'] = 2\n",
    "\n",
    "    #use correleation between parking and material\n",
    "    df.loc[((df['parking'] == 3)|(df['parking'] == 2))& df['material'].isna(),'material'] = 2\n",
    "\n",
    "    #user correlation between area and material\n",
    "    df.loc[(df['area_total'] > 104)& df['material'].isna(),'material'] = 2\n",
    "\n",
    "    #parking\n",
    "    #use coreletion between parking and material\n",
    "    df.loc[((df['material'] == 0)|(df['material'] == 3))& df['parking'].isna(),'parking'] = 1\n",
    "\n",
    "    #use correleation between stories and parking\n",
    "    df.loc[((df['stories'] > 30))& df['parking'].isna(),'parking'] = 0\n",
    "    df.loc[((df['stories'] > 7)&(df['stories'] < 9))& df['parking'].isna(),'parking'] = 1\n",
    "\n",
    "    #use correleation between radius and parking\n",
    "    df.loc[(df['radius'] < 0.0547)& df['parking'].isna(),'parking'] = 0\n",
    "    df.loc[(df['radius'] > 0.251)& df['parking'].isna(),'parking'] = 1\n",
    "\n",
    "    #windows_court and windows_street\n",
    "    #windows_street and balconies\n",
    "    df.loc[(df['balconies'] == 2)& df['windows_street'].isna(),'windows_street'] = 1\n",
    "\n",
    "    #windows street and area\n",
    "    df.loc[(df['area_total'] > 106)& df['windows_street'].isna(),'windows_street'] = 1\n",
    "    df.loc[(df['area_total'] < 47)& df['windows_street'].isna(),'windows_street'] = 0\n",
    "\n",
    "    #windwos_street and stories\n",
    "    df.loc[(df['stories'] > 40)& df['windows_street'].isna(),'windows_street'] = 1\n",
    "    df.loc[(df['stories'] < 9)& df['windows_street'].isna(),'windows_street'] = 0\n",
    "\n",
    "    #windows_street and windows_court\n",
    "    df.loc[(df['windows_street'] == 0)& df['windows_court'].isna(),'windows_court'] = 1\n",
    "\n",
    "    df.loc[((df['windows_street'] == 1)& (df['area_total'] > 75)) & df['windows_court'].isna(),'windows_court'] = 1\n",
    "    df.loc[((df['windows_street'] == 1)& (df['area_total'] < 47)) & df['windows_court'].isna(),'windows_court'] = 0\n",
    "\n",
    "    #balconies\n",
    "    #balconies and stories\n",
    "    df.loc[((df['stories'] >= 12)& (df['area_total'] <= 16)) & df['balconies'].isna(),'balconies'] = 0\n",
    "    df.loc[(df['stories'] < 7) & df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[((df['stories'] > 17)& (df['area_total'] <= 24)) & df['balconies'].isna(),'balconies'] = 0\n",
    "\n",
    "    #matrial and balconies\n",
    "    df.loc[(df['material'] == 0) & df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[(df['material'] == 2) & df['balconies'].isna(),'balconies'] = 0\n",
    "\n",
    "    #balconies and constructed\n",
    "    df.loc[(df['constructed'] < 1970) & df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[(df['constructed'] > 2018) & df['balconies'].isna(),'balconies'] = 0\n",
    "\n",
    "    #loggias\n",
    "    #loggias and balconies\n",
    "    df.loc[(df['balconies'] == 0) & df['loggias'].isna(),'loggias'] = 1\n",
    "    df.loc[(df['balconies'] > 0) & df['loggias'].isna(),'loggias'] = 0\n",
    "\n",
    "    #loggias and stories\n",
    "    df.loc[(df['stories'] < 7) & df['loggias'].isna(),'loggias'] = 0\n",
    "    df.loc[((df['stories'] >= 17)& (df['area_total'] <= 24)) & df['loggias'].isna(),'loggias'] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_radius(df):\n",
    "    #adds radius column to dataframe\n",
    "    df['radius'] = np.sqrt((df['latitude']-55.75)**2 + (df['longitude']-37.55)**2)\n",
    "    return df\n",
    "\n",
    "def group_by_feature_and_price(df, feature, data_train = pd.DataFrame()):\n",
    "    #makes a new column with the mean price for each group to all rows\n",
    "    if data_train.empty:\n",
    "        grouped = df.groupby([feature])\n",
    "\n",
    "        mean_price = grouped['price'].mean()\n",
    "\n",
    "        df_merged = pd.merge(df,mean_price, on=feature, how='left')\n",
    "\n",
    "        column_name = str(feature + '_price')\n",
    "\n",
    "        df[column_name] = df_merged['price_y']\n",
    "    else:\n",
    "        grouped = data_train.groupby([feature])\n",
    "\n",
    "        mean_price = grouped['price'].mean()\n",
    "\n",
    "        df_merged = pd.merge(df,mean_price, on=feature, how='left')\n",
    "\n",
    "        column_name = str(feature + '_price')\n",
    "\n",
    "        df[column_name] = df_merged['price']\n",
    "    return df\n",
    "\n",
    "def one_hot_encode(df, column_to_encode):\n",
    "    \"\"\"one hots encode for one singel column\"\"\"\n",
    "    encoded_df = pd.get_dummies(df[[column_to_encode]].astype(str))\n",
    "    df = pd.concat([df,encoded_df],axis=1)\n",
    "    return df\n",
    "\n",
    "def one_hot_encode_multiple(df, list_of_columns):\n",
    "    \"\"\"takes in multiple columns and runs one hot encode for each column\"\"\"\n",
    "    for column_to_encode in list_of_columns:\n",
    "        #print(column_to_encode)\n",
    "        df = one_hot_encode(df, column_to_encode)\n",
    "    return df\n",
    "\n",
    "def add_direction(df):\n",
    "    \"\"\"adds direction to dataframe, can be one of eight categories (N,S,W,E)\"\"\"\n",
    "    #straight up (north)\n",
    "    normal_vector = np.array([0,1])\n",
    "    #normal_vector = np.tile(normal_vector,(df.shape[0],1))\n",
    "    #normal_vector = normal_vector.reshape((2,-df.shape[0]))\n",
    "    temp = pd.DataFrame()\n",
    "    temp['latitude'] = df['latitude']-55.75\n",
    "    temp['longitude'] = df['longitude']-37.56\n",
    "\n",
    "\n",
    "    apartment_vector = temp[['latitude','longitude']].to_numpy()\n",
    "\n",
    "\n",
    "    #print(np.shape(apartment_vector), np.shape(normal_vector))\n",
    "    angles = []\n",
    "    for vector in apartment_vector:\n",
    "        if vector[0] < 0:\n",
    "            temp_angle = -angle_between(vector,normal_vector)\n",
    "        else:\n",
    "            temp_angle = angle_between(vector,normal_vector)\n",
    "        angles.append(temp_angle)\n",
    "\n",
    "    angles = [element * 10 for element in angles]\n",
    "\n",
    "\n",
    "    angles_series = pd.Series(np.array(angles))\n",
    "    #angles_series.plot.hist()\n",
    "\n",
    "    df['angle'] = angles_series\n",
    "    max = df.angle.max()\n",
    "    min = df.angle.min()\n",
    "    bins = [min,min*7/8,min*5/8,min*3/8,min/8,max/8,max*3/8,max*5/8,max*7/8,max]\n",
    "    rounded_bins = [element for element in bins]\n",
    "    #print(rounded_bins)\n",
    "    direction = pd.cut(df.angle, bins= rounded_bins,labels=['S','SW','W','NW','N','NE','E','SE','S'],ordered=False)\n",
    "    df['direction'] = direction\n",
    "    return df\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\"returns the unit vector if the vector\"\"\"\n",
    "    return vector/np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1,v2):\n",
    "    \"\"\"returns angle between two vectors in radian\"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u,v2_u),-1,1))\n",
    "\n",
    "def fix_geo_data(data_test):\n",
    "    \"\"\"should just be called on test data\"\"\"\n",
    "    #fix all radius issues\n",
    "    data_test._set_value(23,'longitude',37.473761)\n",
    "    data_test._set_value(23,'latitude',55.560891)\n",
    "    data_test._set_value(90,'longitude',37.473761)\n",
    "    data_test._set_value(90,'latitude',55.560891)\n",
    "\n",
    "    data_test._set_value(2511,'longitude',37.478055)\n",
    "    data_test._set_value(2511,'latitude',55.544046)\n",
    "    data_test._set_value(5090,'longitude',37.478055)\n",
    "    data_test._set_value(5090,'latitude',55.544046)\n",
    "    data_test._set_value(6959,'longitude',37.478055)\n",
    "    data_test._set_value(6959,'latitude',55.544046)\n",
    "    data_test._set_value(8596,'longitude',37.478055)\n",
    "    data_test._set_value(8596,'latitude',55.544046)\n",
    "\n",
    "    data_test._set_value(4719,'longitude',37.385493)\n",
    "    data_test._set_value(4719,'latitude',55.853117)\n",
    "\n",
    "    data_test._set_value(9547,'longitude',37.384711)\n",
    "    data_test._set_value(9547,'latitude',55.853511)\n",
    "\n",
    "    data_test._set_value(2529,'longitude',37.464994)\n",
    "    data_test._set_value(2529,'latitude',55.627666)\n",
    "\n",
    "    data_test = add_radius(data_test)\n",
    "\n",
    "    return data_test\n",
    "\n",
    "def drop_n_largest(data_train):\n",
    "    \"\"\"drops 4 largest values, should only be called on training set\"\"\"\n",
    "    #drop 4 largest from training data, maybe not samrt, but we will see\n",
    "    data_train.drop([3217,21414,15840,13938])\n",
    "    return data_train\n",
    "\n",
    "def clean_data(all_data):\n",
    "    \"\"\"cleans the data with all the knowledge we have so far\"\"\"\n",
    "\n",
    "\n",
    "    #fix ceiling issues\n",
    "    all_data.loc[all_data['ceiling'] > 50,'ceiling']*=0.01\n",
    "    all_data.loc[all_data['ceiling'] > 25, 'ceiling']*=0.1\n",
    "    all_data.loc[all_data['ceiling'] < 0.5,'ceiling'] = float('NaN')\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def add_high_up(df):\n",
    "    \"\"\"add exponetital function to determine how high up a building is\"\"\"\n",
    "    high_up = df.floor/df.stories\n",
    "    high_up_exp = np.exp(high_up) - 1\n",
    "    euler = np.exp(1)\n",
    "\n",
    "    df['high_up'] = high_up_exp\n",
    "    df['high_up'].where(df['high_up'] > euler, euler)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def log_radius(df):\n",
    "    df['radius'] = np.log(df['radius'])\n",
    "    return df\n",
    "\n",
    "def cluster_geo_data(df,df_test):\n",
    "    from sklearn.cluster import KMeans\n",
    "    k_means = KMeans(n_clusters = 100, max_iter = 1000, init='k-means++')\n",
    "\n",
    "    lat_long_pairs = df[['latitude','longitude']]\n",
    "    lat_long_pairs_test = df_test[['latitude','longitude']]\n",
    "    target_data = np.log(df.price)/np.log(15)\n",
    "\n",
    "    k_means.fit(lat_long_pairs,sample_weight = target_data)\n",
    "    df['cluster_number'] = k_means.predict(lat_long_pairs)\n",
    "    df_test['cluster_number'] = k_means.predict(lat_long_pairs_test)\n",
    "\n",
    "    return df, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['radius', 'loggias', 'bathrooms_shared', 'balconies', 'id', 'phones', 'rooms', 'area_living', 'ceiling', 'latitude', 'price', 'constructed', 'area_total', 'floor', 'stories', 'area_kitchen', 'bathrooms_private', 'longitude']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23285 entries, 0 to 23284\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   area_total         23285 non-null  float64\n",
      " 1   floor              23285 non-null  float64\n",
      " 2   ceiling            23285 non-null  float64\n",
      " 3   constructed        23285 non-null  float64\n",
      " 4   stories            23285 non-null  float64\n",
      " 5   radius             23285 non-null  float64\n",
      " 6   high_up            23285 non-null  float64\n",
      " 7   angle              23285 non-null  float64\n",
      " 8   direction_N        23285 non-null  uint8  \n",
      " 9   seller_nan         23285 non-null  uint8  \n",
      " 10  condition_2.0      23285 non-null  uint8  \n",
      " 11  condition_3.0      23285 non-null  uint8  \n",
      " 12  district_0.0       23285 non-null  uint8  \n",
      " 13  district_10.0      23285 non-null  uint8  \n",
      " 14  district_11.0      23285 non-null  uint8  \n",
      " 15  district_3.0       23285 non-null  uint8  \n",
      " 16  district_7.0       23285 non-null  uint8  \n",
      " 17  parking_0.0        23285 non-null  uint8  \n",
      " 18  parking_1.0        23285 non-null  uint8  \n",
      " 19  cluster_number_54  23285 non-null  uint8  \n",
      "dtypes: float64(8), uint8(12)\n",
      "memory usage: 1.7 MB\n",
      "None\n",
      "done here\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nan_values = X_train.isna().any()\\n\\nnan_columns = nan_values.any()\\n\\n\\n\\n\\ncolumns_with_nan = X_train.columns[nan_columns].tolist()\\n\\nfor column in X_train.columns:\\n\\n    if X_train[column].isna().any():\\n\\n        print(column)'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, data_test = import_data()\n",
    "\n",
    "Y = data.price\n",
    "test_id = data_test.id\n",
    "\n",
    "radius = True\n",
    "# Add radius\n",
    "if(radius):\n",
    "    data = add_radius(data)\n",
    "    data_test = add_radius(data_test)\n",
    "    #coordinates = ['latitude', 'longitude']\n",
    "    #data = data.drop(columns = coordinates)\n",
    "    #data_test = data_test.drop(columns = coordinates)\n",
    "\n",
    "fix_missing = False\n",
    "if fix_missing:\n",
    "    data = fix_missing_value(data)\n",
    "    data_test = fix_missing_value(data_test)\n",
    "\n",
    "cleaning = True\n",
    "if(cleaning):\n",
    "    data_test = fix_geo_data(data_test)\n",
    "    data = clean_data(data)\n",
    "    data_test = clean_data(data_test)\n",
    "\n",
    "\n",
    "categorical_data = ['seller','layout', 'windows_court', 'windows_street', 'condition', 'building_id','new','district','street',\n",
    "                    'address', 'material', 'elevator_without', 'elevator_passenger', 'elevator_service', 'parking','garbage_chute', 'heating','cluster_number','layout']\n",
    "\n",
    "data_columns = list(data.columns)\n",
    "numerical_data = list(set(data_columns)-set(categorical_data))\n",
    "print(numerical_data)\n",
    "for column in numerical_data:\n",
    "    mean = data[column].mean()\n",
    "    data[column] = data[column].replace(np.NaN, mean)\n",
    "    if column != 'price':\n",
    "        mean_test = data_test[column].mean()\n",
    "        data_test[column] = data_test[column].replace(np.NAN,mean)\n",
    "\n",
    "#Features\n",
    "radius = True\n",
    "district_mean_price = False\n",
    "ohe = True\n",
    "direction = True\n",
    "high_up = True\n",
    "#add high up\n",
    "log_r = False\n",
    "cluster = True\n",
    "irrelevant_features = True\n",
    "\n",
    "if(high_up):\n",
    "    data = add_high_up(data)\n",
    "    data_test = add_high_up(data_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = data.drop(columns = ['area_living', 'area_kitchen'])\n",
    "data_test = data_test.drop(columns = ['area_living', 'area_kitchen'])\n",
    "\n",
    "if(cluster):\n",
    "    data,data_test = cluster_geo_data(data,data_test)\n",
    "\n",
    "if(district_mean_price):\n",
    "    data = group_by_feature_and_price(data,'district')\n",
    "    data_test = group_by_feature_and_price(data_test,'district',data_train=data)\n",
    "    data = data.drop(columns=['price', 'id'])\n",
    "\n",
    "if(direction):\n",
    "    data = add_direction(data)\n",
    "    data_test = add_direction(data_test)\n",
    "    data = one_hot_encode(data, 'direction')\n",
    "    data_test = one_hot_encode(data_test, 'direction')\n",
    "    data = data.drop(columns = 'direction')\n",
    "    data_test = data_test.drop(columns = 'direction')\n",
    "\n",
    "\n",
    "#One hot encode data\n",
    "if(ohe):\n",
    "    encode_categorical_data = ['seller','layout', 'windows_court', 'windows_street', 'condition','new','district',\n",
    "                     'material', 'elevator_without', 'elevator_passenger', 'elevator_service', 'parking','garbage_chute', 'heating','cluster_number']\n",
    "    data = one_hot_encode_multiple(data,encode_categorical_data)\n",
    "    data_test = one_hot_encode_multiple(data_test,encode_categorical_data)\n",
    "\n",
    "if(log_r):\n",
    "    data = log_radius(data)\n",
    "    data_test = log_radius(data_test)\n",
    "\n",
    "if (irrelevant_features):\n",
    "    data = data.drop(columns = ['rooms', 'bathrooms_shared', 'bathrooms_private', 'balconies', 'loggias', 'phones', 'direction_E', 'direction_NE', 'direction_NW', 'direction_S', 'direction_SE', 'direction_SW', 'direction_W', 'direction_nan', 'seller_0.0', 'seller_1.0', 'seller_2.0', 'seller_3.0', 'layout_0.0', 'layout_1.0', 'layout_2.0', 'layout_nan', 'windows_court_0.0', 'windows_court_1.0', 'windows_court_nan', 'windows_street_0.0', 'windows_street_1.0', 'windows_street_nan', 'condition_0.0', 'condition_1.0', 'condition_nan', 'new_0.0', 'new_1.0', 'new_nan', 'district_1.0', 'district_2.0', 'district_4.0', 'district_5.0', 'district_6.0', 'district_8.0', 'district_9.0', 'district_nan', 'material_0.0', 'material_1.0', 'material_2.0', 'material_3.0', 'material_4.0', 'material_5.0', 'material_6.0', 'material_nan', 'elevator_without_0.0', 'elevator_without_1.0', 'elevator_without_nan', 'elevator_passenger_0.0', 'elevator_passenger_1.0', 'elevator_passenger_nan', 'elevator_service_0.0', 'elevator_service_1.0', 'elevator_service_nan', 'parking_2.0', 'parking_nan', 'garbage_chute_0.0', 'garbage_chute_1.0', 'garbage_chute_nan', 'heating_0.0', 'heating_1.0', 'heating_2.0', 'heating_3.0', 'heating_nan', 'cluster_number_0', 'cluster_number_1', 'cluster_number_10', 'cluster_number_11', 'cluster_number_12', 'cluster_number_13', 'cluster_number_14', 'cluster_number_15', 'cluster_number_16', 'cluster_number_17', 'cluster_number_18', 'cluster_number_19', 'cluster_number_2', 'cluster_number_20', 'cluster_number_21', 'cluster_number_22', 'cluster_number_23', 'cluster_number_24', 'cluster_number_25', 'cluster_number_26', 'cluster_number_27', 'cluster_number_28', 'cluster_number_29', 'cluster_number_3', 'cluster_number_30', 'cluster_number_31', 'cluster_number_32', 'cluster_number_33', 'cluster_number_34', 'cluster_number_35', 'cluster_number_36', 'cluster_number_37', 'cluster_number_38', 'cluster_number_39', 'cluster_number_4', 'cluster_number_40', 'cluster_number_41', 'cluster_number_42', 'cluster_number_43', 'cluster_number_44', 'cluster_number_45', 'cluster_number_46', 'cluster_number_47', 'cluster_number_48', 'cluster_number_49', 'cluster_number_5', 'cluster_number_50', 'cluster_number_51', 'cluster_number_52', 'cluster_number_53', 'cluster_number_55', 'cluster_number_56', 'cluster_number_57', 'cluster_number_58', 'cluster_number_59', 'cluster_number_6', 'cluster_number_60', 'cluster_number_61', 'cluster_number_62', 'cluster_number_63', 'cluster_number_64', 'cluster_number_65', 'cluster_number_66', 'cluster_number_67', 'cluster_number_68', 'cluster_number_69', 'cluster_number_7', 'cluster_number_70', 'cluster_number_71', 'cluster_number_72', 'cluster_number_73', 'cluster_number_74', 'cluster_number_75', 'cluster_number_76', 'cluster_number_77', 'cluster_number_78', 'cluster_number_79', 'cluster_number_8', 'cluster_number_80', 'cluster_number_81', 'cluster_number_82', 'cluster_number_83', 'cluster_number_84', 'cluster_number_85', 'cluster_number_86', 'cluster_number_87', 'cluster_number_88', 'cluster_number_89', 'cluster_number_9', 'cluster_number_90', 'cluster_number_91', 'cluster_number_92', 'cluster_number_93', 'cluster_number_94', 'cluster_number_95', 'cluster_number_96', 'cluster_number_97', 'cluster_number_98', 'cluster_number_99'])\n",
    "    data_test = data_test.drop(columns = ['rooms', 'bathrooms_shared', 'bathrooms_private', 'balconies', 'loggias', 'phones', 'direction_E', 'direction_NE', 'direction_NW', 'direction_S', 'direction_SE', 'direction_SW', 'direction_W', 'direction_nan', 'seller_0.0', 'seller_1.0', 'seller_2.0', 'seller_3.0', 'layout_0.0', 'layout_1.0', 'layout_2.0', 'layout_nan', 'windows_court_0.0', 'windows_court_1.0', 'windows_court_nan', 'windows_street_0.0', 'windows_street_1.0', 'windows_street_nan', 'condition_0.0', 'condition_1.0', 'condition_nan', 'new_0.0', 'new_1.0', 'new_nan', 'district_1.0', 'district_2.0', 'district_4.0', 'district_5.0', 'district_6.0', 'district_8.0', 'district_9.0', 'district_nan', 'material_0.0', 'material_1.0', 'material_2.0', 'material_3.0', 'material_4.0', 'material_5.0', 'material_6.0', 'material_nan', 'elevator_without_0.0', 'elevator_without_1.0', 'elevator_without_nan', 'elevator_passenger_0.0', 'elevator_passenger_1.0', 'elevator_passenger_nan', 'elevator_service_0.0', 'elevator_service_1.0', 'elevator_service_nan', 'parking_2.0', 'parking_nan', 'garbage_chute_0.0', 'garbage_chute_1.0', 'garbage_chute_nan', 'heating_0.0', 'heating_1.0', 'heating_2.0', 'heating_3.0', 'heating_nan', 'cluster_number_0', 'cluster_number_1', 'cluster_number_10', 'cluster_number_11', 'cluster_number_12', 'cluster_number_13', 'cluster_number_14', 'cluster_number_15', 'cluster_number_16', 'cluster_number_17', 'cluster_number_18', 'cluster_number_19', 'cluster_number_2', 'cluster_number_20', 'cluster_number_21', 'cluster_number_22', 'cluster_number_23', 'cluster_number_24', 'cluster_number_25', 'cluster_number_26', 'cluster_number_27', 'cluster_number_28', 'cluster_number_29', 'cluster_number_3', 'cluster_number_30', 'cluster_number_31', 'cluster_number_32', 'cluster_number_33', 'cluster_number_34', 'cluster_number_35', 'cluster_number_36', 'cluster_number_37', 'cluster_number_38', 'cluster_number_39', 'cluster_number_4', 'cluster_number_40', 'cluster_number_41', 'cluster_number_42', 'cluster_number_43', 'cluster_number_44', 'cluster_number_45', 'cluster_number_46', 'cluster_number_47', 'cluster_number_48', 'cluster_number_49', 'cluster_number_5', 'cluster_number_50', 'cluster_number_51', 'cluster_number_52', 'cluster_number_53', 'cluster_number_55', 'cluster_number_56', 'cluster_number_57', 'cluster_number_58', 'cluster_number_59', 'cluster_number_6', 'cluster_number_60', 'cluster_number_61', 'cluster_number_62', 'cluster_number_63', 'cluster_number_64', 'cluster_number_65', 'cluster_number_66', 'cluster_number_67', 'cluster_number_68', 'cluster_number_69', 'cluster_number_7', 'cluster_number_70', 'cluster_number_71', 'cluster_number_72', 'cluster_number_73', 'cluster_number_74', 'cluster_number_75', 'cluster_number_76', 'cluster_number_77', 'cluster_number_78', 'cluster_number_79', 'cluster_number_8', 'cluster_number_80', 'cluster_number_81', 'cluster_number_82', 'cluster_number_83', 'cluster_number_84', 'cluster_number_85', 'cluster_number_86', 'cluster_number_87', 'cluster_number_88', 'cluster_number_89', 'cluster_number_9', 'cluster_number_90', 'cluster_number_91', 'cluster_number_92', 'cluster_number_93', 'cluster_number_94', 'cluster_number_95', 'cluster_number_96', 'cluster_number_97', 'cluster_number_98', 'cluster_number_99'])\n",
    "\n",
    "#Drop cat_data\n",
    "data = data.drop(columns = categorical_data)\n",
    "data = data.drop(columns = ['latitude','longitude','price', 'id'])\n",
    "data_test = data_test.drop(columns=['id','latitude','longitude'])\n",
    "data_test = data_test.drop(columns = categorical_data)\n",
    "print(data.info())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=42)\n",
    "print('done here')\n",
    "\n",
    "\n",
    "overlaps=0\n",
    "for index, value in X_train.items():\n",
    "    for idx, value in X_test.items():\n",
    "        if index==idx:\n",
    "            overlaps+=1\n",
    "            break\n",
    "\n",
    "print(overlaps)\n",
    "\n",
    "\n",
    "'''nan_values = X_train.isna().any()\n",
    "\n",
    "nan_columns = nan_values.any()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "columns_with_nan = X_train.columns[nan_columns].tolist()\n",
    "\n",
    "for column in X_train.columns:\n",
    "\n",
    "    if X_train[column].isna().any():\n",
    "\n",
    "        print(column)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    assert (y_true >= 0).all()\n",
    "    assert (y_pred >= 0).all()\n",
    "    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n",
    "    return np.mean(log_error ** 2) ** 0.5\n",
    "\n",
    "\n",
    "kaggle=False\n",
    "if(kaggle):\n",
    "    Gradient_boosting_model = GradientBoostingRegressor(random_state=0)\n",
    "    Y = np.log(Y)/np.log(15)\n",
    "    Gradient_boosting_model.fit(data,Y)\n",
    "    result = Gradient_boosting_model.predict(data_test)\n",
    "\n",
    "else:\n",
    "    param = {\"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.001],\n",
    "    \"min_samples_split\": [2],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"max_depth\":[3],\n",
    "    \"max_features\":[\"auto\"],\n",
    "    \"criterion\": [\"friedman_mse\"],\n",
    "    \"n_estimators\":[50]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1398445008856409\n",
      "zero_count:  0\n",
      "irrelevant features:  ['cluster_number_54']\n"
     ]
    }
   ],
   "source": [
    "if kaggle:\n",
    "    prediction_kaggle = 15**prediction_kaggle\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id'] = test_id\n",
    "    submission['price_prediction'] = prediction_kaggle\n",
    "    submission.to_csv('submission_XGB_EDA.csv', index= False)\n",
    "\n",
    "else:\n",
    "    import matplotlib.pyplot as plt\n",
    "    '''prediction = 15**prediction\n",
    "    rmsle = root_mean_squared_log_error(y_test,prediction)\n",
    "    print(\"first run\", rmsle)'''\n",
    "\n",
    "    Gradient_boosting_model = GradientBoostingRegressor(min_samples_split = 3, min_samples_leaf= 1, max_depth= 6, n_estimators= 729, learning_rate= 0.15823029511056225) #beste by default lr=0.1, max_depth=8, n_estimators=750 => 0.137\n",
    "    #'min_samples_split': 3, 'min_samples_leaf': 1, 'max_depth': 6, 'n_estimators': 729, 'learning_rate': 0.15823029511056225\n",
    "    y_train_new = np.log(y_train)/np.log(15)\n",
    "    Gradient_boosting_model.fit(X_train, y_train_new)\n",
    "    prediction = 15**Gradient_boosting_model.predict(X_test)\n",
    "\n",
    "    print(root_mean_squared_log_error(y_test, prediction))\n",
    "\n",
    "\n",
    "    features = Gradient_boosting_model.feature_names_in_\n",
    "    importances = Gradient_boosting_model.feature_importances_\n",
    "    zero_count=0\n",
    "    irrelevant=[]\n",
    "    for i in range(len(importances)):\n",
    "        if importances[i]<0.001:\n",
    "            irrelevant.append(features[i])\n",
    "    print('zero_count: ',zero_count)\n",
    "    print('irrelevant features: ', irrelevant)\n",
    "\n",
    "    def objective(trial):\n",
    "        Gradient_boosting_model = GradientBoostingRegressor(min_samples_split=trial.suggest_int('min_samples_split',2,4), min_samples_leaf=trial.suggest_int('min_samples_leaf', 1,3),max_depth=trial.suggest_int('max_depth',5,8), n_estimators=trial.suggest_int('n_estimators', 400,750), learning_rate=trial.suggest_float('learning_rate',0.05, 0.20)) #beste by default lr=0.1, max_depth=8, n_estimators=750 => 0.137\n",
    "        #'min_samples_split': 3, 'min_samples_leaf': 1, 'max_depth': 6, 'n_estimators': 729, 'learning_rate': 0.15823029511056225\n",
    "        y_train_new = np.log(y_train)/np.log(15)\n",
    "        Gradient_boosting_model.fit(X_train, y_train_new)\n",
    "        prediction = 15**Gradient_boosting_model.predict(X_test)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_axes([0,0,1,1])\n",
    "        features = Gradient_boosting_model.feature_names_in_\n",
    "        importances = Gradient_boosting_model.feature_importances_\n",
    "        ax.bar(features,importances)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return root_mean_squared_log_error(y_test, prediction)\n",
    "\n",
    "    #study = optuna.create_study(direction = 'minimize')\n",
    "    #study.optimize(objective, n_trials=1)\n",
    "    #0.1363254710562304"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30d84d53d57d1b7ba70473b78335f4453acd6aaed45e0ab38ecc893be9e416d9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
