{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import string\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from moscow_housing.display_data import import_data\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook we will do EDA (explanatory data analysis). We will look at dataset standard statistics, individual feature analysis and finding any relations or trends considering multiple features. As a conlusion we will suggest features which can be added as a result of EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import data\n",
    "data_train, data_test = import_data()\n",
    "\n",
    "#sometimes we have to look at all the data togheter\n",
    "all_data = pd.concat([data_train,data_test],ignore_index=True)\n",
    "\n",
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Now lets look at all the basics of the data\n",
    "print('Number of Training Examples = {}'.format(data_train.shape[0]))\n",
    "print('Number of Test Examples = {}\\n'.format(data_test.shape[0]))\n",
    "print('Training X Shape = {}'.format(data_train.shape))\n",
    "print('Training y Shape = {}\\n'.format(data_train['price'].shape[0]))\n",
    "print('Test X Shape = {}'.format(data_test.shape))\n",
    "print('Test y Shape = {}\\n'.format(data_test.shape[0]))\n",
    "print('Train columns \\n', list(data_train.columns))\n",
    "print('Test columns \\n', list(data_test.columns))\n",
    "\n",
    "#check type of each column, and see how many duplicates we have:\n",
    "for column in data_train.columns:\n",
    "    print('\\n - type of column: ', data_train.dtypes[column],'\\n - number of null values: ', data_train[column].isnull().sum(), '\\n - number of unique inputs: ', data_train[column].value_counts().count(),'\\n', data_train[column].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see, the training and test set have the same number of columns except for price. Lets look at little bit closer at these features.\n",
    "### **Features description**\n",
    "* `id` - Unique ID for apartment\n",
    "    - type of column:  int64\n",
    "    - number of null values:  0\n",
    "    - number of unique inputs:  23285\n",
    "* `seller` - The type of apartment seller\n",
    " - type of column:  float64\n",
    " - number of null values:  8830\n",
    " - number of unique inputs:  4\n",
    "* `price` - The listed price of the apartment (TARGET: only available in train)\n",
    " - type of column:  float64\n",
    " - number of null values:  0\n",
    " - number of unique inputs:  9504\n",
    "* `area_total` - Total area of the apartment\n",
    " - type of column:  float64\n",
    " - number of null values:  0\n",
    " - number of unique inputs:  3249\n",
    "* `area_kitchen` - Total kitchen area in the apartment\n",
    " - type of column:  float64\n",
    " - number of null values:  4721\n",
    " - number of unique inputs:  385\n",
    "* `area_living` - Total living space area in the apartment\n",
    " - type of column:  float64\n",
    " - number of null values:  3882\n",
    " - number of unique inputs:  1041\n",
    "* `floor`- Primary building floor of the apartment\n",
    " - type of column:  float64\n",
    " - number of null values:  0\n",
    " - number of unique inputs:  84\n",
    "* `rooms` - Number of rooms in the apartment\n",
    " - type of column:  float64\n",
    " - number of null values:  0\n",
    " - number of unique inputs:  6\n",
    "* `layout` - Overal apartment layout\n",
    " - type of column:  float64\n",
    " - number of null values:  17642\n",
    " - number of unique inputs:  3\n",
    "* `ceiling` - Ceiling height in the apartment\n",
    " - type of column:  float64\n",
    " - number of null values:  11093\n",
    " - number of unique inputs:  121\n",
    "* `bathrooms_shared` - Number of shared bathrooms\n",
    " - type of column:  float64\n",
    " - number of null values:  3872\n",
    " - number of unique inputs:  5\n",
    "* `bathrooms_private` - Number of private bathrooms\n",
    " - type of column:  float64\n",
    " - number of null values:  3872\n",
    " - number of unique inputs:  5\n",
    "* `windows_court` - Whether the apartment has windows facing a courtyard\n",
    " - type of column:  float64\n",
    " - number of null values:  8072\n",
    " - number of unique inputs:  2\n",
    "* `windows_street` - Whether the apartment has windows facing a street\n",
    " - type of column:  float64\n",
    " - number of null values:  8072\n",
    " - number of unique inputs:  2\n",
    "* `balconies` - Number of balconies in the apartment\n",
    " - type of column:  float64\n",
    " - number of null values:  10520\n",
    " - number of unique inputs:  5\n",
    "* `loggias`- Number of loggias in the apartment (balcony-like, google it)\n",
    " - type of column:  float64\n",
    " - number of null values:  10520\n",
    " - number of unique inputs:  5\n",
    "* `condition` - Current condition of the apartment\n",
    " - type of column:  float64\n",
    " - number of null values:  9810\n",
    " - number of unique inputs:  4\n",
    "* `phones` - Number of phone numbers associated with the advert\n",
    " - type of column:  float64\n",
    " - number of null values:  80\n",
    " - number of unique inputs:  3\n",
    "* `building_id` - ID used to map apartments to buildings\n",
    " - type of column:  int64\n",
    " - number of null values:  0\n",
    " - number of unique inputs:  6791\n",
    "* `new` - Whether it is an old or new building\n",
    " - type of column:  float64\n",
    " - number of null values:  264\n",
    " - number of unique inputs:  2\n",
    "* `latitude` - Latitude coordinate of building\n",
    " - type of column:  float64\n",
    " - number of null values:  0\n",
    " - number of unique inputs:  6208\n",
    "* `longitude` - Longitude coordinate of building\n",
    " - type of column:  float64\n",
    " - number of null values:  0\n",
    " - number of unique inputs:  6159\n",
    "* `district` - Administriative district within Moscow\n",
    " - type of column:  float64\n",
    " - number of null values:  130\n",
    " - number of unique inputs:  12\n",
    "* `street` - Bulding street name\n",
    " - type of column:  object\n",
    " - number of null values:  0\n",
    " - number of unique inputs:  1682\n",
    "* `address` - Building address (within street)\n",
    " - type of column:  object\n",
    " - number of null values:  0\n",
    " - number of unique inputs:  1851\n",
    "* `constructed` - Year when the building was constructed\n",
    " - type of column:  float64\n",
    " - number of null values:  794\n",
    " - number of unique inputs:  128\n",
    "* `material` - Primary building material used in building\n",
    " - type of column:  float64\n",
    " - number of null values:  3972\n",
    " - number of unique inputs:  7\n",
    "* `stories` - Total number of floors in the building\n",
    " - type of column:  float64\n",
    " - number of null values:  0\n",
    " - number of unique inputs:  62\n",
    "* `elevator_without` - Whether the building has apartments without elevator access\n",
    " - type of column:  float64\n",
    " - number of null values:  357\n",
    " - number of unique inputs:  2\n",
    "* `elevator_passenger` - Whether the building has apartments with passenger elevator access\n",
    " - type of column:  float64\n",
    " - number of null values:  357\n",
    " - number of unique inputs:  2\n",
    "* `elevator_service` - Whether the building has apartments with service elevator access\n",
    " - type of column:  float64\n",
    " - number of null values:  357\n",
    " - number of unique inputs:  2\n",
    "* `parking` - Parking options for building\n",
    " - type of column:  float64\n",
    " - number of null values:  6788\n",
    " - number of unique inputs:  3\n",
    "* `garbage_chute` - Whether the building has a garbage chute system\n",
    " - type of column:  float64\n",
    " - number of null values:  8811\n",
    " - number of unique inputs:  2\n",
    "* `heating` - Primary heating system used in the building\n",
    " - type of column:  float64\n",
    " - number of null values:  8161\n",
    " - number of unique inputs:  4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Categorical features**\n",
    "seller, layout, windows_court, windows_street, condition, phones, building_id, new, distircit, street and address, constructed, material, elevator, parking_options, garbage_chute and heating.\n",
    "\n",
    "All of these features will just mislead the model if we put them into it right away. They need to be worked with, and all of them are maybe not that important.\n",
    "For example windows_court, models will not understand that this is actually a binary feature.\n",
    "\n",
    "In lightGBM we can specify which parameters are categorical features, https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html, for more reading about this topic.\n",
    "Will explore if using this on all features instead of grouping some is more valuable.\n",
    "\n",
    "\n",
    "\n",
    "# **Functions**\n",
    "Lets start by making some of the functions I will use to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_multiple(df, list_of_columns):\n",
    "    \"\"\"takes in multiple columns and runs one hot encode for each column\"\"\"\n",
    "    for column_to_encode in list_of_columns:\n",
    "        print(column_to_encode)\n",
    "        df = one_hot_encode(df, column_to_encode)\n",
    "    return df\n",
    "\n",
    "def one_hot_encode(df, column_to_encode):\n",
    "    \"\"\"one hots encode for one singel column\"\"\"\n",
    "    encoded_df = pd.get_dummies(df[[column_to_encode]].astype(str))\n",
    "    print(encoded_df.info())\n",
    "    df = pd.concat([df,encoded_df],axis=1)\n",
    "    return df\n",
    "\n",
    "def add_high_up(df):\n",
    "    \"\"\"add exponetital function to determine how high up a building is\"\"\"\n",
    "    high_up = df.floor/df.stories\n",
    "    high_up_exp = np.exp(high_up) - 1\n",
    "    euler = np.exp(1)\n",
    "\n",
    "    df['high_up'] = high_up_exp\n",
    "    df['high_up'].where(df['high_up'] > euler, euler)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_radius(df):\n",
    "    \"\"\"adds radius column to dataframe\"\"\"\n",
    "    df['radius'] = np.sqrt((df['latitude']-55.75)**2 + (df['longitude']-37.56)**2)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_direction(df):\n",
    "    \"\"\"adds direction to dataframe, can be one of eight categories (N,S,W,E)\"\"\"\n",
    "    #straight up (north)\n",
    "    normal_vector = np.array([0,1])\n",
    "    #normal_vector = np.tile(normal_vector,(df.shape[0],1))\n",
    "    #normal_vector = normal_vector.reshape((2,-df.shape[0]))\n",
    "    temp = pd.DataFrame()\n",
    "    temp['latitude'] = df['latitude']-55.75\n",
    "    temp['longitude'] = df['longitude']-37.56\n",
    "\n",
    "\n",
    "    apartment_vector = temp[['latitude','longitude']].to_numpy()\n",
    "\n",
    "\n",
    "    #print(np.shape(apartment_vector), np.shape(normal_vector))\n",
    "    angles = []\n",
    "    for vector in apartment_vector:\n",
    "        if vector[0] < 0:\n",
    "            temp_angle = -angle_between(vector,normal_vector)\n",
    "        else:\n",
    "            temp_angle = angle_between(vector,normal_vector)\n",
    "        angles.append(temp_angle)\n",
    "\n",
    "    angles = [element * 10 for element in angles]\n",
    "\n",
    "\n",
    "    angles_series = pd.Series(np.array(angles))\n",
    "\n",
    "    df['direction'] = angles_series\n",
    "    max = df.direction.max().round()\n",
    "    min = df.direction.min().round()\n",
    "\n",
    "    bins = [min,min*7/8,min*5/8,min*3/8,min/8,max/8,max*3/8,max*5/8,max*7/8,max]\n",
    "    rounded_bins = [element.round() for element in bins]\n",
    "    #print(rounded_bins)\n",
    "\n",
    "    direction = pd.cut(df.direction, bins= rounded_bins,labels=['S','SW','W','NW','N','NE','E','SE','S'],ordered=False)\n",
    "    df['direction'] = direction\n",
    "    return df\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\"returns the unit vector if the vector\"\"\"\n",
    "    return vector/np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1,v2):\n",
    "    \"\"\"returns angle between two vectors in radian\"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u,v2_u),-1,1))\n",
    "\n",
    "def pair_plot(df, columns):\n",
    "    #plots a pairplot with the sns libary\n",
    "    sns.pairplot(df[columns],\n",
    "    plot_kws={'alpha': 0.6},\n",
    "    diag_kws={'bins': 30})\n",
    "\n",
    "def plot_correlation(df):\n",
    "    #plots correlation in a dataframe\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(\n",
    "    corr,\n",
    "    linewidths=.5,\n",
    "    annot=True,\n",
    "    fmt='.1f'\n",
    "    )\n",
    "\n",
    "def group_by_feature_and_price(df, feature, data_train = pd.DataFrame()):\n",
    "    #makes a new column with the mean price for each group to all rows\n",
    "    if data_train.empty:\n",
    "        grouped = df.groupby([feature])\n",
    "\n",
    "        mean_price = grouped['price'].mean()\n",
    "\n",
    "        df_merged = pd.merge(df,mean_price, on=feature, how='left')\n",
    "\n",
    "        column_name = str(feature + '_price')\n",
    "\n",
    "        df[column_name] = df_merged['price_y']\n",
    "    else:\n",
    "        grouped = data_train.groupby([feature])\n",
    "\n",
    "        mean_price = grouped['price'].mean()\n",
    "\n",
    "        df_merged = pd.merge(df,mean_price, on=feature, how='left')\n",
    "\n",
    "        column_name = str(feature + '_price')\n",
    "\n",
    "        df[column_name] = df_merged['price']\n",
    "    return df\n",
    "\n",
    "def fix_radius(data_test):\n",
    "    \"\"\"should just be called on test data\"\"\"\n",
    "    #fix all radius issues\n",
    "    data_test._set_value(23,'radius',0.203899)\n",
    "    data_test._set_value(90,'radius',0.203899)\n",
    "    data_test._set_value(2511,'radius',0.218159)\n",
    "    data_test._set_value(5090,'radius',0.218159)\n",
    "    data_test._set_value(6959,'radius',0.218159)\n",
    "    data_test._set_value(8596,'radius',0.218159)\n",
    "    data_test._set_value(4719, 'radius',0.19580)\n",
    "    data_test._set_value(9547, 'radius',0.19520)\n",
    "    data_test._set_value(2529, 'radius', np.sqrt((37.464994-37.55)**2+(55.627666-55.75)**2))\n",
    "\n",
    "    return data_test\n",
    "\n",
    "def fix_geo_data(data_test):\n",
    "    \"\"\"should just be called on test data\"\"\"\n",
    "    #fix all radius issues\n",
    "    data_test._set_value(23,'longitude',37.473761)\n",
    "    data_test._set_value(23,'latitude',55.560891)\n",
    "    data_test._set_value(90,'longitude',37.473761)\n",
    "    data_test._set_value(90,'latitude',55.560891)\n",
    "\n",
    "    data_test._set_value(2511,'longitude',37.478055)\n",
    "    data_test._set_value(2511,'latitude',55.544046)\n",
    "    data_test._set_value(5090,'longitude',37.478055)\n",
    "    data_test._set_value(5090,'latitude',55.544046)\n",
    "    data_test._set_value(6959,'longitude',37.478055)\n",
    "    data_test._set_value(6959,'latitude',55.544046)\n",
    "    data_test._set_value(8596,'longitude',37.478055)\n",
    "    data_test._set_value(8596,'latitude',55.544046)\n",
    "\n",
    "    data_test._set_value(4719,'longitude',37.385493)\n",
    "    data_test._set_value(4719,'latitude',55.853117)\n",
    "\n",
    "    data_test._set_value(9547,'longitude',37.384711)\n",
    "    data_test._set_value(9547,'latitude',55.853511)\n",
    "\n",
    "    data_test._set_value(2529,'longitude',37.464994)\n",
    "    data_test._set_value(2529,'latitude',55.627666)\n",
    "\n",
    "    data_test = add_radius(data_test)\n",
    "\n",
    "    return data_test\n",
    "\n",
    "def cluster_geo_data(df,df_test):\n",
    "    from sklearn.cluster import KMeans\n",
    "    k_means = KMeans(n_clusters = 30, max_iter = 1000, init='k-means++')\n",
    "\n",
    "    lat_long_pairs = df[['latitude','longitude']]\n",
    "    lat_long_pairs_test = df_test[['latitude','longitude']]\n",
    "    target_data = np.log(df.price)/np.log(15)\n",
    "\n",
    "    k_means.fit(lat_long_pairs,sample_weight = target_data)\n",
    "    df['cluster_number'] = k_means.predict(lat_long_pairs, sample_weight = target_data)\n",
    "    df_test['cluster_number'] = k_means.predict(lat_long_pairs_test)\n",
    "\n",
    "    return df, df_test\n",
    "\n",
    "\n",
    "def drop_n_largest(data_train):\n",
    "    \"\"\"drops 4 largest values, should only be called on training set\"\"\"\n",
    "    #drop 4 largest from training data, maybe not samrt, but we will see\n",
    "    data_train.drop([3217,21414,15840,13938])\n",
    "\n",
    "def clean_data(all_data):\n",
    "    \"\"\"cleans the data with all the knowledge we have so far\"\"\"\n",
    "\n",
    "\n",
    "    #fix ceiling issues\n",
    "    all_data.loc[all_data['ceiling'] > 50,'ceiling']*=0.01\n",
    "    all_data.loc[all_data['ceiling'] > 25, 'ceiling']*=0.1\n",
    "    all_data.loc[all_data['ceiling'] < 0.5,'ceiling'] = float('NaN')\n",
    "\n",
    "    #fix area_kitchen and area_living issues\n",
    "    all_data['living'] = all_data.area_living/all_data.area_total\n",
    "    all_data['kitchen'] = all_data.area_total/all_data.area_kitchen\n",
    "\n",
    "    all_data.loc[all_data['living'] > 1,'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['living'] > 1,'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "\n",
    "    all_data.loc[all_data.area_living/all_data.area_total > 1, 'area_living'] = float('NaN')\n",
    "\n",
    "    all_data['living'] = all_data.area_living/all_data.area_total\n",
    "    all_data['kitchen'] = all_data.area_kitchen/all_data.area_total\n",
    "\n",
    "    all_data['sum_area'] = all_data.area_living + all_data.area_kitchen\n",
    "    all_data.loc[all_data['sum_area'] == 100, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 100, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "\n",
    "    #this fixed some few rows.\n",
    "    #when printing we see some other very clear \"precentage situations, fixing these\n",
    "    all_data.loc[all_data['sum_area'] == 38.5, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 38.5, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 52.7, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 52.7, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 71.6, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 71.6, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 20), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 20), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 15), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 15), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 10), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 10), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 30), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 30), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 10), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 10), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 15), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 15), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 20), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 20), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 25), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 25), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 30), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 30), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 50), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 50), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 60) & (all_data['area_total'] > 120), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 60) & (all_data['area_total'] > 120), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] < 70) & (all_data['area_total'] > 120)&(all_data['rooms'] < 3), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] < 70) & (all_data['area_total'] > 120)&(all_data['rooms'] < 3), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def plot_categorical(df, column1, column2):\n",
    "    CrosstabResult=pd.crosstab(index=df[column1],columns=df[column2])\n",
    "\n",
    "    CrosstabResult.plot.bar(figsize=(7,4), rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Pricing**\n",
    "Pricing is our goal target, lets start by looking at how it is compared to other factors, and if we can see any interesting things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#data_train = add_direction(data_train)\n",
    "#data_train = one_hot_encode_multiple(data_train,['windows_street','windows_court','layout','district','direction'])\n",
    "#data_train.info()\n",
    "\n",
    "#log_data['price'] = np.log10(data_train.price)\n",
    "#log_data.plot.hist(bins= 500)\n",
    "#vi har utrolig lite små data, vil ha nærmere normalfordeling.\n",
    "\n",
    "# plot correleation of data to see which rows to focus on\n",
    "#all_data.corr().style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Adding values to data**\n",
    "### **Radius**\n",
    "We saw that longitude and latitude didnt have an impact, lets see if we can change this into radius and check the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#add radius to test, all data and training data\n",
    "data_train = add_radius(data_train)\n",
    "data_test = add_radius(data_test)\n",
    "all_data = add_radius(all_data)\n",
    "\n",
    "#all_data['direction'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Grouping features to make more powerfull features**\n",
    "#### **Districts**\n",
    "I think that there could be some value in grouping the different districts, to get the average price in that district as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#group data by district and look at correleation for new feature\n",
    "data_train = group_by_feature_and_price(data_train,'district')\n",
    "data_test = group_by_feature_and_price(data_test, 'district', data_train=data_train)\n",
    "all_data = group_by_feature_and_price(all_data, 'district')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Street**\n",
    "There is also normal that a street has some sort of similar price or is in the same pricerange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#group data by street\n",
    "data_train = group_by_feature_and_price(data_train,'street')\n",
    "data_test = group_by_feature_and_price(data_test, 'street', data_train=data_train)\n",
    "all_data = group_by_feature_and_price(all_data, 'street')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Building**\n",
    "Apartments in the same building often has the same price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#group data by building_id\n",
    "data_train = group_by_feature_and_price(data_train,'building_id')\n",
    "data_test = group_by_feature_and_price(data_test, 'building_id', data_train=data_train)\n",
    "all_data = group_by_feature_and_price(all_data, 'building_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Constructed year**\n",
    "Lets check if the year an apparment was constructed can affect the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_train = group_by_feature_and_price(data_train,'constructed')\n",
    "data_test = group_by_feature_and_price(data_test, 'constructed', data_train=data_train)\n",
    "all_data = group_by_feature_and_price(all_data, 'constructed')\n",
    "\n",
    "all_data['high_up'] = all_data.floor/all_data.stories\n",
    "all_data['high_up'].where(all_data['high_up'] >= 1, 1)\n",
    "data_train['high_up'] = data_train.floor/data_train.stories\n",
    "data_test['high_up'] = data_test.floor/data_test.stories\n",
    "data_train['high_up'].where(data_train['high_up'] >= 1, 1)\n",
    "data_test['high_up'].where(data_test['high_up'] >= 1, 1)\n",
    "print(data_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Conclusion**\n",
    "Grouping lead to us being able to use data which is actually categorical, like building_id. This model will not be able to understand what it means, the model doesnt understand the data.\n",
    "\n",
    "To make the model understand we have two possibilities, turn it into categories (easy for few number of categories). However when we have huge categories we need to make data which the model can understand => grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#lets check the result, for example how they correlate with price, and also chek their pair plots\n",
    "all_data.corr().style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pair_plot(data_train,['price', 'area_total','district_price','street_price','building_id_price','constructed_price','high_up'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Constructed year and mean constructed price**\n",
    "This pair comparison tells us a little bit about how the price is affected over time. In the start it is quite random, but as the time goes on we see a quite clear pattern. The price for houses constructed in the middle years are really low. Could this say something about the cuality of the buildings from this era?\n",
    "\n",
    "Moving on if we look at this feature comparied to the histogram plot of constructed year. We can see that there we have data about very few old houses, and these have random values. However the more data we get the better the prediction is getting less and less random.\n",
    "The year something is constructed is not an integer, it is an category. But looking at this plot, we may be able to intepreter it as a category.\n",
    "\n",
    "\n",
    "on the pair plots we can see that a lot of the data is affected by some extreme values, lets see what happens when we remove these values, we will also have to calculate new means()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#find n largest prices\n",
    "n_largest = data_train.nlargest(4,'price')['price']\n",
    "print(n_largest)\n",
    "\n",
    "#removes 4 larges inputs to get better data\n",
    "train_dropped = data_train.drop([3217,21414,15840,13938])\n",
    "all_dropped = all_data.drop([3217,21414,15840,13938])\n",
    "\n",
    "train_dropped = group_by_feature_and_price(train_dropped,'district')\n",
    "all_dropped = group_by_feature_and_price(all_dropped, 'district')\n",
    "train_dropped = group_by_feature_and_price(train_dropped,'street')\n",
    "all_dropped = group_by_feature_and_price(all_dropped, 'street')\n",
    "train_dropped = group_by_feature_and_price(train_dropped,'building_id')\n",
    "all_dropped = group_by_feature_and_price(all_dropped, 'building_id')\n",
    "\n",
    "#lets check new correlation and compare with previous\n",
    "all_dropped.corr().style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that district_price is not affected, showing that this feature is stable and can withstans extreme values\n",
    "street and building dropped a little bit, but maybe its not that important. However the pair plots might look different now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pair_plot(train_dropped,['price', 'area_total','district_price','street_price','building_id_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The piar plots does now look a little bit better, at least they are more zoomed in.\n",
    "### **Use of information**\n",
    "Lets check if our new features can be used in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('number of common buildind_id:\\n',data_train['building_id'].isin(data_test['building_id']).sum())\n",
    "print('common streets in both sets:\\n',data_train[['street']].isin(data_test[['street']]).sum())\n",
    "print('number of unique streets:', len(all_data['street'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see there is not any common buildings in the test set and train set. Therefor average building price is not important\n",
    "Unless we can use some other features to train the average bulding price this information should not be used in our model\n",
    "\n",
    "We can also see that the there are only 27 common streets in the two sets, but over 1800 unique streets combined. This information is usefull for some of the rows in the test set, but not for everyone.\n",
    "\n",
    "However we still want to use information about the building, how can we accomplish this.\n",
    "\n",
    "#### **Building price continue**\n",
    "What else is characteristic for a building? We want to use the information from a certain type of building to give information about what these types of buildings is worth.\n",
    "Is newer buildings more expnesive for example?\n",
    "Lets try to group by building id and plot this with some information, to see what can be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Outliers**\n",
    "\n",
    "Now lets look at the outliers, lets start looking at the radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_value = data_train['radius'].max()\n",
    "min_value = data_train['radius'].min()\n",
    "\n",
    "print('rows with missing latitude and longitude in test data: \\n ',data_test[['longitude','latitude']][data_test['longitude'].isnull() == True])\n",
    "\n",
    "max_value_test = data_test['radius'].nlargest(10)\n",
    "print('max radius from trainingset: \\n',max_value, '\\n10 highest values from test set:\\n',max_value_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the code above, the test set is missing 2 longitude/latitude entries\n",
    "there is also 6 houses which is outsie moscow, have to remove radius for these buildings and look at other factors\n",
    "Want to look at the rows which is missing longitude and latitude, do they have anything else which connects the to moscow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "row 23 and 90 is missing radius\n",
    "\n",
    "rows outside moscov:\n",
    "2511    106.579234\n",
    "5090    106.579234\n",
    "6959    106.579234\n",
    "8596    106.579234\n",
    "4719     96.021654\n",
    "9547     96.012910\n",
    "2529     39.132054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print(data_with_radius.loc[[23]]['street'])\n",
    "#print(data_with_radius.loc[[90]]['building_id'])\n",
    "#print(data_with_radius[data_with_radius['street']=='пос. Коммунарка'])\n",
    "data_test._set_value(23,'radius',0.203899)\n",
    "data_test._set_value(90,'radius',0.203899)\n",
    "\n",
    "\n",
    "\n",
    "#Now we dont have any missing radius values in the test data\n",
    "#lets look at the rows outside of moscov\n",
    "data_test._set_value(2511,'radius',0.218159)\n",
    "data_test._set_value(5090,'radius',0.218159)\n",
    "data_test._set_value(6959,'radius',0.218159)\n",
    "data_test._set_value(8596,'radius',0.218159)\n",
    "\n",
    "#print(data_test.loc[[2511]][['street']])\n",
    "#print(data_test.loc[[5090]][['longitude','latitude']])\n",
    "#print(data_test.loc[[6959]])\n",
    "#print(data_test.loc[[8596]])\n",
    "#print(data_with_radius[data_with_radius['street']=='Бунинские Луга ЖК'])\n",
    "\n",
    "#print(data_test.loc[[4719]][['street','address']])\n",
    "#print(data_with_radius[['address','radius']][data_with_radius['street']=='улица Центральная'])\n",
    "\n",
    "data_test._set_value(4719, 'radius',0.19580)\n",
    "data_test._set_value(9547, 'radius',0.19520)\n",
    "\n",
    "#print(data_test.loc[[2529]][['street','address','district']])\n",
    "#print(data_with_radius[['longitude','latitude']][data_with_radius['street']=='улица 1-я Линия'])\n",
    "data_test._set_value(2529, 'radius', np.sqrt((37.464994-37.55)**2+(55.627666-55.75)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Outliers continue**\n",
    "Now we have fixed everythin for radius, lets look at the most extreme prices in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "largest_area = data_train.nlargest(3, columns='area_total')\n",
    "largest_area_test = data_test.nlargest(5, columns='area_total')['area_total']\n",
    "print('largest total area test : \\n',largest_area_test,'\\n','largest area train: \\n', largest_area[['price','area_total']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that the three highest values from the training set is almost twice teh size of the training set\n",
    "We have earlier seen that the plots was better when removing these. Lets check out if it is possible that this data is relevant for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#look at these particular rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(largest_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The two largest ones is in the same building, same size, only difference I see is that there are different types of rooms.\n",
    "However this could be usefull if building_mean_price could be used, as it states that this is a expensive building, however i think that this data is just noise for the model, with regards to the test data.\n",
    "\n",
    "### **Oldest and Newest building for sale for each set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oldest_train = data_train.nsmallest(10, columns='constructed')['constructed']\n",
    "oldest_test = data_test.nsmallest(10, columns='constructed')['constructed']\n",
    "\n",
    "youngest_train = data_train.nlargest(10, columns='constructed')['constructed']\n",
    "youngest_test = data_test.nlargest(10, columns='constructed')['constructed']\n",
    "print(oldest_train,oldest_test,youngest_train,youngest_test)\n",
    "#as we can see they span over the same tiemframe, meaning that there are no outliers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Ceiling**\n",
    "When testing, with a model, we experienced that ceiling was behaving a little bit strange. Lets look into this and look for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#make boxplot\n",
    "#all_data.boxplot(column= 'ceiling')\n",
    "#data_train.boxplot(column= 'ceiling')\n",
    "#data_test.boxplot(column= 'ceiling')\n",
    "\n",
    "#locate cm values:\n",
    "all_data_ceiling_cm = all_data[all_data['ceiling'] > 50]\n",
    "\n",
    "#all_data_ceiling_cm.boxplot(column = 'ceiling')\n",
    "#this data is consistent, and could be turned into m\n",
    "all_data.loc[all_data['ceiling'] > 50,'ceiling']*=0.01\n",
    "\n",
    "\n",
    "#now we have som entries which is between 25 and 30\n",
    "all_data_ceiling_error = all_data[all_data['ceiling'] > 25]\n",
    "#all_data_ceiling_error.boxplot(column = 'ceiling')\n",
    "#print(all_data_ceiling_error[['price','ceiling','area_total']])\n",
    "\n",
    "#i think this data is given in desimeter, and could be converted into cm\n",
    "all_data.loc[all_data['ceiling'] > 25, 'ceiling']*=0.1\n",
    "\n",
    "#Now we can try to give values to the ones ranging from 3.5 to 8. Some of them is probably correct.\n",
    "#lets firs look at maximum value\n",
    "#all_data_ceiling_6 = all_data[all_data['ceiling']>6.4]\n",
    "#found two duplicate rows, except from room.\n",
    "#The highest observed ceiling height which i belive is 3,5 m, and smallest is 2.40.\n",
    "#Lets assume that ceiling heigh can be up towards 4 meters in the most luxurious apartmens.\n",
    "#then for example ceiling heights ranging from 8 to 5 meters could possibly be two story buildings.\n",
    "#the solution here is just adding the average height of the two floors into one\n",
    "#2.2m is minimum, 2.4m is standard and 2.6m is considered good according to internet\n",
    "#if we take this into account, a ceiling of 8 m should be a three story building:\n",
    "\n",
    "#all_data.loc[all_data['ceiling'] > 7.2, 'ceiling']*=0.3333\n",
    "\n",
    "#the buildings between 7.2 and 4.6 should be considered as two story buildings\n",
    "#all_data.loc[all_data['ceiling'] > 4.6,'ceiling']*=0.5\n",
    "\n",
    "#highest_now = all_data['ceiling'].argmax()\n",
    "#highest_row = all_data.loc[highest_now]\n",
    "#print(highest_row)\n",
    "\n",
    "#there is some null values, these should be replaced with nan\n",
    "all_data.loc[all_data['ceiling'] < 0.5,'ceiling'] = float('NaN')\n",
    "all_data.boxplot(column='ceiling')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can clearly see that there is a big difference in how the data is entered. And that it is the same problem both in test and train.\n",
    "We see three ways of entering ceiling data, one in cm and two distinct group in cm.\n",
    "The goal is to get all the data in m.\n",
    "\n",
    "Now all the data is in the same range, but there is still quit big difference. We think this is because there is differnet number of floors in each appartment.\n",
    "Because there is no information about number of floors for each apartment, we think it is good to keep these differences, so that\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **LIVING AREA AND KITCHEN AREA**\n",
    "When testing models the model aslo did worse when we trained them with living and kitchen area included. Therefore we think it might be something wrong in how thtis data is presented.\n",
    "Here we will look at the kitchen and living area compared to total area"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data['living'] = all_data.area_living/all_data.area_total\n",
    "all_data['kitchen'] = all_data.area_total/all_data.area_kitchen\n",
    "#all_data.boxplot(column='living')\n",
    "#I expected this number to be less than 1, looking at the boxplot this doesnt give much sense\n",
    "\n",
    "all_data_living_1 = all_data[all_data['living'] > 1]\n",
    "\n",
    "#looking at this data this is all very small apparments. As some columns is very close to sum living and kitchen to 1, i think all of these are in precentage.\n",
    "#the ones who dont add up have a private bathroom, meaning that this takes up the rest of the space\n",
    "#solution is to multiply. However, how do we find these errors among the bigger appartmens.\n",
    "\n",
    "#print(all_data[all_data.index.duplicated()])\n",
    "all_data.loc[all_data['living'] > 1,'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[all_data['living'] > 1,'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "\n",
    "#There are errors in row 842, 12595,13573, 25924. Here area living is bigger than area_total, therefore i will just put NaN\n",
    "all_data.loc[all_data.area_living/all_data.area_total > 1, 'area_living'] = float('NaN')\n",
    "\n",
    "all_data['living'] = all_data.area_living/all_data.area_total\n",
    "all_data['kitchen'] = all_data.area_kitchen/all_data.area_total\n",
    "\n",
    "#Now we have no obvious errors, but i know there are still some errors in these columns, lets look at the ones with a really low score.\n",
    "\n",
    "#all_data_small_living = all_data[all_data['living'] < 0.2]\n",
    "#all_data_big_living = all_data[all_data['living'] > 0.9]\n",
    "#print(all_data_small_living,'\\n',all_data_big_living)\n",
    "#there is a clear tendency that there are still a lot of precentage in the data. It is quite clear that if the sum of area_kitchen and area_living sums up to 100, it is given in precentage\n",
    "all_data['sum_area'] = all_data.area_living + all_data.area_kitchen\n",
    "all_data.loc[all_data['sum_area'] == 100, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[all_data['sum_area'] == 100, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "\n",
    "#this fixed some few rows.\n",
    "#when printing we see some other very clear \"precentage situations, fixing these\n",
    "all_data.loc[all_data['sum_area'] == 38.5, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[all_data['sum_area'] == 38.5, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[all_data['sum_area'] == 52.7, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[all_data['sum_area'] == 52.7, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[all_data['sum_area'] == 71.6, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[all_data['sum_area'] == 71.6, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 20), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 20), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 15), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 15), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 10), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 10), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 30), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 30), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 10), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 10), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 15), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 15), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 20), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 20), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 25), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 25), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 30), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 30), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 50), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 50), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 60) & (all_data['area_total'] > 120), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] == 60) & (all_data['area_total'] > 120), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] < 70) & (all_data['area_total'] > 120)&(all_data['rooms'] < 3), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "all_data.loc[(all_data['sum_area'] < 70) & (all_data['area_total'] > 120)&(all_data['rooms'] < 3), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_data['sum_area'] = all_data.area_living + all_data.area_kitchen\n",
    "all_data['living'] = all_data.area_living/all_data.area_total\n",
    "all_data['kitchen'] = all_data.area_kitchen/all_data.area_total\n",
    "\n",
    "#there is some cases with sum_area bigger than total area, meaning that one of the inputs is wrong. In these cases we will just put nan\n",
    "#need to figure out which gives best results, which change\n",
    "all_data.loc[all_data['sum_area'] > all_data['area_total'], 'area_living'] = float('NaN')\n",
    "\n",
    "all_data_small_living = all_data[all_data['living'] < 0.3]\n",
    "all_data_big_living = all_data[all_data['living'] > 0.8]\n",
    "print(all_data_small_living[['area_total','area_living','area_kitchen','sum_area','rooms']],'\\n',all_data_big_living[['area_total','area_living','area_kitchen','sum_area','rooms']])\n",
    "n = 10\n",
    "#print(all_data[all_data['kitchen'] > 0.5][['area_total','area_living','area_kitchen','sum_area','rooms']])\n",
    "#print(all_data['sum_area'].value_counts()[:n].index.tolist())\n",
    "#print(all_data[(all_data['sum_area'] < 70) & (all_data['area_total'] > 160)][['area_total','area_living','area_kitchen','sum_area','rooms']])\n",
    "#as we can see there is four rows with higher area living than area total, i think the solution here is to\n",
    "\n",
    "all_data.boxplot(column='kitchen')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ***Outliers square price**\n",
    "\n",
    "Lets intorduce a new feature, called square price. Will check for outliers in training set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Missing data**\n",
    "Some colums have a lot of misssing data, lets look at a way for each column.\n",
    "Layout is the colums with the most missing data, lets take a lot at that"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layout_data = data_train['layout']\n",
    "print('number of missing values: ',layout_data.isnull().sum())\n",
    "\n",
    "#check correleation between layout and other data\n",
    "layout_corr = data_train.corr()\n",
    "print(layout_corr['layout'].nlargest(4))\n",
    "#layout_corr.style.background_gradient(cmap='coolwarm')\n",
    "\n",
    "#didnt find anythin, will check difference in price among nan values and thos who actually have floor\n",
    "mean_price_without_nan = data_train[~data_train['layout'].isnull()]['price'].mean()\n",
    "mean_price_with_nan = data_train[data_train['layout'].isnull()]['price'].mean()\n",
    "\n",
    "print(mean_price_without_nan,mean_price_with_nan)\n",
    "#these prices ar3e quit similar, lets check year.\n",
    "\n",
    "mean_year_without_nan = data_train[~data_train['layout'].isnull()]['constructed'].mean()\n",
    "mean_year_with_nan = data_train[data_train['layout'].isnull()]['constructed'].mean()\n",
    "print(mean_year_without_nan,mean_year_with_nan)\n",
    "#as we can see there are quite strong evidence that there is a correleation betweem if layous is mising or not, and building year\n",
    "\n",
    "#From my understanding, the second most important feature is the size of the living room, lets see how many null values we have here\n",
    "print('trainig set:', data_train['area_living'].isnull().sum(),'\\ntest set: ', data_test['area_living'].isnull().sum())\n",
    "\n",
    "#There is a lot of missing values, how can we fix this, check if we group by building, how good the correleation betweem actual and buding average\n",
    "grouped_by_building = data_train.groupby(['building_id'])\n",
    "mean_area_living = grouped_by_building['area_living'].mean()\n",
    "print(mean_area_living)\n",
    "\n",
    "data_with_group = pd.merge(data_train,mean_area_living, on='building_id', how='left')\n",
    "\n",
    "data_test['mean_area_living'] = data_with_group['area_living_y']\n",
    "\n",
    "corr = data_test.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')\n",
    "#this did not have a high correlation, and can not be used. Lets try and make a function for fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Missing Data continue**\n",
    "\n",
    "If there is missing data in the categorical data, ant the data is very turned towards one side, it could be good to just assume thahtthe rest of the data also is like that.\n",
    "\n",
    "Lets start by plotting all of the different categorical data.\n",
    "categorical_data = ['seller','layout', 'windows_court', 'windows_street', 'condition', 'building_id','new','district','street',\n",
    "                    'address', 'material', 'elevator_without', 'elevator_passenger', 'elevator_service', 'parking','garbage_chute', 'heating']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To compare categorical and integer data, we need to conecrt the integer data to categorical."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data['area_groups'] = pd.qcut(all_data.area_total, q=6)\n",
    "all_data['ceiling_groups'] = pd.qcut(all_data.ceiling, q=8)\n",
    "all_data['radius_groups'] = pd.qcut(all_data.radius, q=8)\n",
    "data_train['price_groups'] = pd.qcut(data_train.price, q=6)\n",
    "all_data['stories_groups'] = pd.qcut(all_data.stories, q=10)\n",
    "all_data['floor_groups'] = pd.qcut(all_data.floor, q=6)\n",
    "all_data['constructed_groups'] = pd.qcut(all_data.constructed, q=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#seller\n",
    "all_data['seller'].value_counts().plot(kind='bar')\n",
    "#seller has a nice distribution, cant send everything into one group.\n",
    "plot_categorical(all_data,'seller','new')\n",
    "plot_categorical(all_data,'seller','constructed_groups')\n",
    "#can clearly see that if new is 1, then seller is 3. If new is 0, then seller is not clear.\n",
    "plot_categorical(all_data,'district','seller')\n",
    "plot_categorical(all_data, 'radius_groups','seller')\n",
    "#quite clear, in district 11 and 0 the seller is mainly 3, however in district 6 and 3, the seller is 1.\n",
    "#in short radius, the seller is 3, in high radius the seller is 1.\n",
    "\n",
    "plot_categorical(all_data, 'stories_groups','seller')\n",
    "plot_categorical(all_data, 'condition','seller')\n",
    "#seller type 3 doesnt care to input condition. Condition is missing 14000 values, seller type 3 has 12000 (soon more values, sick)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#layout\n",
    "all_data['layout'].value_counts().plot(kind='bar')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#layout and constructed\n",
    "plot_categorical(all_data,'layout','constructed')\n",
    "#too little data so say somehting smart. Seems like quite the same except for 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#condition and layout\n",
    "plot_categorical(all_data,'condition','layout')\n",
    "#very similar, not good"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#windowsCourt\n",
    "\n",
    "all_data_2 = all_data[all_data['windows_street']==1]\n",
    "all_data_2['windows_court'].value_counts().plot(kind='bar')\n",
    "\n",
    "plot_categorical(all_data_2,'stories_groups','windows_court')\n",
    "plot_categorical(all_data_2,'radius_groups','windows_court')\n",
    "plot_categorical(all_data_2,'area_groups','windows_court')\n",
    "plot_categorical(all_data_2,'material','windows_court')\n",
    "plot_categorical(all_data_2,'parking','windows_court')\n",
    "plot_categorical(all_data_2,'constructed_groups','windows_court')\n",
    "plot_categorical(all_data_2,'condition','windows_court')\n",
    "plot_categorical(all_data_2,'new','windows_court')\n",
    "plot_categorical(all_data_2,'balconies','windows_court')\n",
    "plot_categorical(all_data_2,'loggias','windows_court')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Windows street\n",
    "all_data['windows_street'].value_counts().plot(kind='bar')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#window street and window court, very corellated, can be used!!!!\n",
    "plot_categorical(all_data,'windows_court', 'windows_street')\n",
    "#best inromation here is that if there is not a window by the court, there is 100% a window at the street\n",
    "plot_categorical(all_data,'windows_street', 'windows_court')\n",
    "#here we can again see that if there is no window to the street, there is a windows court\n",
    "#can be used if we can learn one of them, and then predict the other.\n",
    "plot_categorical(all_data,'stories_groups','windows_street')\n",
    "plot_categorical(all_data,'radius_groups','windows_street')\n",
    "plot_categorical(all_data,'area_groups','windows_street')\n",
    "plot_categorical(all_data,'material','windows_street')\n",
    "plot_categorical(all_data,'parking','windows_street')\n",
    "plot_categorical(all_data,'constructed_groups','windows_street')\n",
    "plot_categorical(all_data,'condition','windows_street')\n",
    "plot_categorical(all_data,'new','windows_street')\n",
    "plot_categorical(all_data,'balconies','windows_street')\n",
    "plot_categorical(all_data,'loggias','windows_street')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#condition, some clear tnedency\n",
    "all_data['condition'].value_counts().plot(kind='bar')\n",
    "#very equal, need do find something smart to cahnge this.\n",
    "\n",
    "plot_categorical(all_data,'new', 'condition')\n",
    "plot_categorical(all_data,'constructed_groups', 'condition')\n",
    "plot_categorical(all_data, 'condition','constructed_groups')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#balconies and loggias\n",
    "all_data['balconies'].value_counts().plot(kind='bar')\n",
    "#no clear tendency\n",
    "plot_categorical(all_data, 'stories_groups', 'balconies')\n",
    "plot_categorical(all_data, 'constructed_groups', 'balconies')\n",
    "plot_categorical(all_data, 'district', 'balconies')\n",
    "plot_categorical(all_data, 'radius_groups', 'balconies')\n",
    "plot_categorical(all_data, 'material', 'balconies')\n",
    "plot_categorical(all_data, 'windows_street', 'balconies')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data['loggias'].value_counts().plot(kind='bar')\n",
    "#no clear tendency\n",
    "\n",
    "plot_categorical(all_data,'balconies','loggias')\n",
    "plot_categorical(all_data, 'stories_groups', 'loggias')\n",
    "plot_categorical(all_data, 'floor_groups', 'loggias')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#condition, togehter with building year\n",
    "plot_categorical(all_data,'condition','constructed')\n",
    "#we can see that most new building are of condition one, for the rest not so clear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#new, quite clear\n",
    "all_data['new'].value_counts().plot(kind='bar')\n",
    "#quite clear tendency, will be even better with some other information"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#new and constructed\n",
    "plot_categorical(all_data,'new','constructed')\n",
    "#good data, can use new to predict constructed and vice versa to fix."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#new and seller\n",
    "plot_categorical(all_data,'new','seller')\n",
    "#very good, can be used to predict new and vice versa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#new and layout\n",
    "plot_categorical(all_data,'new','layout')\n",
    "# totally ok, will probably use somehting else to predict new, and layout is mostly just one category."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#new and condition\n",
    "plot_categorical(all_data,'new','condition')\n",
    "#totally ok, can be used to predict that new almost just has condition 0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#material\n",
    "all_data['material'].value_counts().plot(kind='bar')\n",
    "#quite clear which is most common, if you have som additional information could be good.\n",
    "\n",
    "plot_categorical(all_data,'new','material')\n",
    "plot_categorical(all_data,'condition','material')\n",
    "plot_categorical(all_data,'elevator_without','material')\n",
    "plot_categorical(all_data,'constructed_groups','material')\n",
    "plot_categorical(all_data,'district','material')\n",
    "plot_categorical(all_data,'parking','material')\n",
    "plot_categorical(all_data,'stories_groups','material')\n",
    "plot_categorical(all_data,'area_groups','material')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#material and layout\n",
    "plot_categorical(all_data,'layout','material')\n",
    "#layout is to equal to one to be affected"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#material and new\n",
    "plot_categorical(all_data,'new','material')\n",
    "#can be used to be quite sure that new buildings is constructed with material number 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#material and condtion\n",
    "plot_categorical(all_data,'condition','material')\n",
    "#quite good news, clear correleation that material 2 gives condtion 0, and material 3 gives condtion 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#'elevator_without', 'elevator_passenger', 'elevator_service', 'parking',\n",
    "# 'garbage_chute', 'heating']\n",
    "#plot one for each elevator, elevator without\n",
    "all_data['elevator_without'].value_counts().plot(kind='bar')\n",
    "#not very clear, cant be so sure about this one, just states the there are appartmens without elevator access within the building\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#elevator passenger\n",
    "all_data['elevator_passenger'].value_counts().plot(kind='bar')\n",
    "#very nice! can assume tha almost everybody has a elevator passenger, should find out what categorizises the one who dont have"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#elevator service\n",
    "all_data['elevator_service'].value_counts().plot(kind='bar')\n",
    "#totally ok information, but is not super clear. however gives a good indication if we can find somethign else"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#without and passegner togheter\n",
    "plot_categorical(all_data,'elevator_without','elevator_passenger')\n",
    "#if there is no appartmens without elevator, then there is almost certainly a passegner elevator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#without and service\n",
    "plot_categorical(all_data,'elevator_without','elevator_service')\n",
    "#no good correleation, very similar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#service and passegner\n",
    "plot_categorical(all_data,'elevator_passenger','elevator_service')\n",
    "#again quite good, if there is no passenger elevator there is no service elevator. If there is a passegner elevator, there is most likelt a service elevator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#we can check elevator without again, as this is the most unclear one.\n",
    "plot_categorical(all_data,'elevator_without','garbage_chute')\n",
    "#with new: some information, conditon:more information, material:more information(just one type)\n",
    "#seller:some information, parking:more information+, (heating,garbage none)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#elevator Without was best found through elevator passegner, so lets check this.\n",
    "plot_categorical(all_data, 'elevator_passenger','parking')\n",
    "#parking:totally ok, if there is no elevator passenger, most likely 1, if parking is 2, then there is elevator passegner\n",
    "#condtion:bad, new: if there is no passegner, its not new, material:some information\n",
    "#if seller i 3 then passegner 1, (heating and garbage_chute bad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#'parking',\n",
    "all_data['parking'].value_counts().plot(kind='bar')\n",
    "#not so much information, but parking 2 can make some exiting results in others. lets check this\n",
    "plot_categorical(all_data,'new','parking')\n",
    "plot_categorical(all_data,'material','parking')\n",
    "plot_categorical(all_data,'elevator_without','parking')\n",
    "plot_categorical(all_data,'stories_groups','parking')\n",
    "plot_categorical(all_data,'radius_groups','parking')\n",
    "plot_categorical(all_data,'parking','radius_groups')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#parking and layout (layout has a lot of missing values)\n",
    "plot_categorical(all_data,'parking','layout')\n",
    "#conclusion: layout is most likely 1 anyways, would be interesting to see if there is something to tell if 0 and 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#parking and new\n",
    "plot_categorical(all_data,'new','parking')\n",
    "#conclusion: if parking is 2, most likely new 0. but not that clear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#parking and condtion\n",
    "plot_categorical(all_data,'material','parking')\n",
    "#if parking is 0, then material is 2, if material is 3, then parking is 1. material 2, most likely parking 0\n",
    "#seller:not good,"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#parking and conditon\n",
    "plot_categorical(all_data,'condition', 'parking')\n",
    "#if condition is 1, then parking is 1, if condtion is 0, then parking could be 0 (same for condtion 3)\n",
    "#material, condition, seller, windows??"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#garbage_chute\n",
    "all_data['garbage_chute'].value_counts().plot(kind='bar')\n",
    "#most likely 1 for this one, but there is no correlation betweem anyone"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#garbage_chute and heating\n",
    "plot_categorical(all_data,'heating', 'garbage_chute')\n",
    "#if heating is 2, then garbage_chute is 0, else it is not very good"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#heating\n",
    "all_data['heating'].value_counts().plot(kind='bar')\n",
    "#heating is most likely 0 anyway. Could be interesting to see if there is something with the 1,2,3 heating."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#rooms\n",
    "all_data['rooms'].value_counts().plot(kind='bar')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#rooms and layout\n",
    "plot_categorical(all_data,'rooms', 'layout')\n",
    "#again, layout is just to bad to say somethin about others."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#rooms and parking, and rooms and new\n",
    "plot_categorical(all_data,'parking', 'rooms')\n",
    "#the more rooms, the higher chance for 0 parking.\n",
    "\n",
    "plot_categorical(all_data, 'rooms','new')\n",
    "#the more rooms, the lower chance of being new. But mostlt is not new, so would be interesting to find somehting which the new ones had on common.\n",
    "\n",
    "plot_categorical(all_data, 'rooms','ceiling_groups')\n",
    "#material:bad, condition:bad, floor and stories:bad\n",
    "\n",
    "#check compared to area\n",
    "plot_categorical(all_data, 'rooms','area_groups')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bathrooms_priavte\n",
    "all_data['bathrooms_private'].value_counts().plot(kind='bar')\n",
    "#ikke noe spesielt å henter her"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#bathrooms shared\n",
    "all_data['bathrooms_shared'].value_counts().plot(kind='bar')\n",
    "#ikke noe spesielt her heller"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bathrooms_priavte and bathrooms_shared\n",
    "plot_categorical(all_data, 'bathrooms_shared','bathrooms_private')\n",
    "#very good as expected, lets see if there is any way to precit the two others"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check compared to area\n",
    "plot_categorical(all_data,'area_groups','bathrooms_private')\n",
    "#low area, most likely no private bathroom\n",
    "\n",
    "plot_categorical(all_data,'area_groups','bathrooms_shared')\n",
    "#low area, most likely shared bathroom, everybodt has a bathroom"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#ceiling\n",
    "all_data['ceiling_groups'].value_counts().plot(kind='bar')\n",
    "\n",
    "#ceiling groups and layout, condtion and material\n",
    "plot_categorical(all_data,'ceiling_groups','material')\n",
    "plot_categorical(all_data,'area_groups','ceiling_groups')\n",
    "plot_categorical(all_data,'ceiling_groups','condition')\n",
    "plot_categorical(all_data,'new','ceiling_groups')\n",
    "plot_categorical(all_data,'ceiling_groups','rooms')\n",
    "plot_categorical(all_data,'ceiling_groups','parking')\n",
    "\n",
    "#Layout:bad, material:good, most likely material 2 if high ceiling, material 3 if super low\n",
    "#if low ceiling, condtion 1, if cieling in 3.0-3.1, condtion 0\n",
    "#if material 4, then low ceiling."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data = add_direction(all_data)\n",
    "#all_data['district'].value_counts().plot(kind='bar')\n",
    "#plot_categorical(all_data,'district','radius_groups')\n",
    "#plot_categorical(all_data[all_data['radius'] < 0.02], 'direction','district')\n",
    "#if there is no missing values in a column, this can be used to guess other columns, if there is a lot of missing values, these columns should be guessed\n",
    "#plot_categorical()\n",
    "\n",
    "\n",
    "all_data.info()\n",
    "#layout is by"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Use of found information**\n",
    "Lets try to use the information we just discovered\n",
    "I want to mainly use this information to predict missing values.\n",
    "\n",
    "I will start by summing up what i learned, and then use this to make rules.\n",
    "\n",
    "Layout has by far the most missing values, and there is no correleation with other things to detect anything deifferent than that we can just put all the missing values in layout to 1.\n",
    "\n",
    "Next up is balconies and loggias. From my plot you can see that if you dont have a loggias, you have most likely one balcony.\n",
    "However if you have any loggias, you dont have balconies."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data_2 = all_data\n",
    "def fix_missing_value(df):\n",
    "    #layout, set everythin to 1\n",
    "    df['layout'].fillna(int(1.0),inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    #found correleation between number of stories and balconies,loggias\n",
    "    df.loc[(df['stories'] < 7.0)&df['loggias'].isna(),'loggias'] = 0\n",
    "    df.loc[(df['stories'] < 7.0)&df['balconies'].isna(),'balconies'] = 1\n",
    "\n",
    "    df.loc[((df['stories'] > 12) & (df['stories'] < 16))|((df['stories'] > 17) & (df['stories'] < 24))&(df['loggias'].isna()),'loggias'] = 1\n",
    "    df.loc[((df['stories'] > 12) & (df['stories'] < 16))|((df['stories'] > 17) & (df['stories'] < 24))&(df['loggias'].isna()),'balconies'] = 0\n",
    "\n",
    "\n",
    "    #use the correleation between balconies and loggias to replace nans,\n",
    "    df.loc[(df['loggias'] == 0.0 )& df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[(df['balconies'] == 0.0 )& df['loggias'].isna(),'loggias'] = 1\n",
    "    df.loc[(df['loggias'] > 0.0 )& df['balconies'].isna(),'balconies'] = 0\n",
    "    df.loc[(df['balconies'] > 0.0 )& df['loggias'].isna(),'loggias'] = 0\n",
    "\n",
    "    #use correlation between material and ceiling groups\n",
    "    ceiling_under_2 = len(df[((df['material'] == 4)|(df['material'] == 3)) & df['ceiling'].isna()])\n",
    "    sample = df[df['ceiling'] < 2.6]['ceiling'].sample(n=ceiling_under_2,replace=True).to_numpy()\n",
    "    df.loc[((df['material'] == 4)|(df['material'] == 3)) & (df['ceiling'].isna()),'ceiling'] = sample\n",
    "    #use corelation between condition and ceiling groups\n",
    "    ceiling_under_2 = len(df[(df['condition'] == 1) & df['ceiling'].isna()])\n",
    "    sample = df[df['ceiling'] < 2.6]['ceiling'].sample(n=ceiling_under_2,replace=True).to_numpy()\n",
    "    df.loc[(df['condition'] == 1) & df['ceiling'].isna(),'ceiling'] = sample\n",
    "\n",
    "\n",
    "    #use correleation betwwen ceiling and area\n",
    "    ceiling_over_3 = len(df[(df['area_total'] > 104)&(df['ceiling'].isna())])\n",
    "    sample = df[df['ceiling'] > 3.0]['ceiling'].sample(n=ceiling_over_3,replace=True).to_numpy()\n",
    "    df.loc[(df['area_total'] > 104)&(df['ceiling'].isna() ),'ceiling'] = sample\n",
    "\n",
    "    ceiling_over_3 = len(df[(df['area_total'] > 75)&(df['ceiling'].isna())])\n",
    "    sample = df[df['ceiling'] > 2.95]['ceiling'].sample(n=ceiling_over_3,replace=True).to_numpy()\n",
    "    df.loc[(df['area_total'] > 75)&(df['ceiling'].isna() ),'ceiling'] = sample\n",
    "\n",
    "    ceiling_under_2 = len(df[(df['area_total'] < 55)&(df['ceiling'].isna())])\n",
    "    sample = df[df['ceiling'] < 2.8]['ceiling'].sample(n=ceiling_under_2,replace=True).to_numpy()\n",
    "    df.loc[(df['area_total'] < 55)&(df['ceiling'].isna() ),'ceiling'] = sample\n",
    "\n",
    "    #seller\n",
    "    #use correleation between seller and new\n",
    "    df.loc[(df['new'] == 1 )& df['seller'].isna(),'seller'] = 3\n",
    "\n",
    "    #use correleation between seller and constructed\n",
    "    df.loc[(df['constructed'] > 2018)& df['seller'].isna(),'seller'] = 3\n",
    "    df.loc[(df['constructed'] < 1990)& df['seller'].isna(),'seller'] = 1\n",
    "\n",
    "    #use correleation between seller and stories\n",
    "    df.loc[(df['stories'] > 17)& df['seller'].isna(),'seller'] = 3\n",
    "    df.loc[(df['stories'] < 9)& df['seller'].isna(),'seller'] = 1\n",
    "\n",
    "    #use correleation between seller and district\n",
    "    df.loc[((df['district'] == 3)|(df['district']==6))& df['seller'].isna(),'seller'] = 1\n",
    "    df.loc[((df['district'] == 11)|(df['district']==2))& df['seller'].isna(),'seller'] = 3\n",
    "\n",
    "    #conditon\n",
    "    #use correleation between condition and new\n",
    "    df.loc[(df['new'] == 1)& df['condition'].isna(),'condition'] = 0\n",
    "\n",
    "    #use correleation between condition and constructed year\n",
    "    df.loc[(df['constructed'] >= 2017)& df['condition'].isna(),'condition'] = 0\n",
    "    df.loc[(df['constructed'] <= 1989)& df['condition'].isna(),'condition'] = 1\n",
    "\n",
    "    #material\n",
    "    #use correlation between material and district\n",
    "    df.loc[(df['district'] == 3)& df['material'].isna(),'material'] = 3\n",
    "\n",
    "    #use correleation between material and constructed year\n",
    "    df.loc[((df['constructed'] > 1970)& (df['constructed'] <= 1989))&df['material'].isna(),'material'] = 3\n",
    "    df.loc[(df['constructed'] <= 1970)&df['material'].isna(),'material'] = 0\n",
    "\n",
    "    #use correleation between stories and material\n",
    "    df.loc[(df['stories'] < 7)& df['material'].isna(),'material'] = 0\n",
    "    df.loc[((df['stories'] > 7)&(df['stories'] < 9))& df['material'].isna(),'material'] = 3\n",
    "\n",
    "    #use correleation between conditon and material\n",
    "    df.loc[(df['condition'] == 3)& df['material'].isna(),'material'] = 3\n",
    "    df.loc[(df['condition'] == 0)& df['material'].isna(),'material'] = 2\n",
    "\n",
    "    #use correleation between parking and material\n",
    "    df.loc[((df['parking'] == 3)|(df['parking'] == 2))& df['material'].isna(),'material'] = 2\n",
    "\n",
    "    #user correlation between area and material\n",
    "    df.loc[(df['area_total'] > 104)& df['material'].isna(),'material'] = 2\n",
    "\n",
    "    #parking\n",
    "    #use coreletion between parking and material\n",
    "    df.loc[((df['material'] == 0)|(df['material'] == 3))& df['parking'].isna(),'parking'] = 1\n",
    "\n",
    "    #use correleation between stories and parking\n",
    "    df.loc[((df['stories'] > 30))& df['parking'].isna(),'parking'] = 0\n",
    "    df.loc[((df['stories'] > 7)&(df['stories'] < 9))& df['parking'].isna(),'parking'] = 1\n",
    "\n",
    "    #use correleation between radius and parking\n",
    "    df.loc[(df['radius'] < 0.0547)& df['parking'].isna(),'parking'] = 0\n",
    "    df.loc[(df['radius'] > 0.251)& df['parking'].isna(),'parking'] = 1\n",
    "\n",
    "    #windows_court and windows_street\n",
    "    #windows_street and balconies\n",
    "    df.loc[(df['balconies'] == 2)& df['windows_street'].isna(),'windows_street'] = 1\n",
    "\n",
    "    #windows street and area\n",
    "    df.loc[(df['area_total'] > 106)& df['windows_street'].isna(),'windows_street'] = 1\n",
    "    df.loc[(df['area_total'] < 47)& df['windows_street'].isna(),'windows_street'] = 0\n",
    "\n",
    "    #windwos_street and stories\n",
    "    df.loc[(df['stories'] > 40)& df['windows_street'].isna(),'windows_street'] = 1\n",
    "    df.loc[(df['stories'] < 9)& df['windows_street'].isna(),'windows_street'] = 0\n",
    "\n",
    "    #windows_street and windows_court\n",
    "    df.loc[(df['windows_street'] == 0)& df['windows_court'].isna(),'windows_court'] = 1\n",
    "\n",
    "    df.loc[((df['windows_street'] == 1)& (df['area_total'] > 75)) & df['windows_court'].isna(),'windows_court'] = 1\n",
    "    df.loc[((df['windows_street'] == 1)& (df['area_total'] < 47)) & df['windows_court'].isna(),'windows_court'] = 0\n",
    "\n",
    "    #balconies\n",
    "    #balconies and stories\n",
    "    df.loc[((df['stories'] >= 12)& (df['area_total'] <= 16)) & df['balconies'].isna(),'balconies'] = 0\n",
    "    df.loc[(df['stories'] < 7) & df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[((df['stories'] > 17)& (df['area_total'] <= 24)) & df['balconies'].isna(),'balconies'] = 0\n",
    "\n",
    "    #matrial and balconies\n",
    "    df.loc[(df['material'] == 0) & df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[(df['material'] == 2) & df['balconies'].isna(),'balconies'] = 0\n",
    "\n",
    "    #balconies and constructed\n",
    "    df.loc[(df['constructed'] < 1970) & df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[(df['constructed'] > 2018) & df['balconies'].isna(),'balconies'] = 0\n",
    "\n",
    "    #loggias\n",
    "    #loggias and balconies\n",
    "    df.loc[(df['balconies'] == 0) & df['loggias'].isna(),'loggias'] = 1\n",
    "    df.loc[(df['balconies'] > 0) & df['loggias'].isna(),'loggias'] = 0\n",
    "\n",
    "    #loggias and stories\n",
    "    df.loc[(df['stories'] < 7) & df['loggias'].isna(),'loggias'] = 0\n",
    "    df.loc[((df['stories'] >= 17)& (df['area_total'] <= 24)) & df['loggias'].isna(),'loggias'] = 1\n",
    "\n",
    "    #bathrooms shared, bathroomes private\n",
    "\n",
    "    #loggias and blaconies\n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "#all_data_2 = fix_missing_value(all_data_2)\n",
    "\n",
    "\n",
    "#all_data_2.info()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Missing data Imputation**\n",
    "The work done above did not help out solution at all, it actually made it worse. Lets try using imputation from sklearn instead. Will also add a missing value column for each missing value.\n",
    "These columns is already one_hot_encoded and should not be a part of one_hot_encoding.\n",
    "They should also be added as catergorical features in light_gbm.\n",
    "Should be done after data is cleaned from clear errors.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "data,data_test = import_data()\n",
    "all_data = pd.concat([data,data_test])\n",
    "#make a copyt o avoid changing data\n",
    "new_data = all_data.copy()\n",
    "new_data.columns = all_data.columns\n",
    "\n",
    "#should not be based on price\n",
    "new_data = new_data.drop(columns=['price','area_kitchen','area_living','street','address'])\n",
    "\n",
    "#find which columns has missing values, indicating which columns should be imputed\n",
    "missing_cols = (col for col in new_data.columns\n",
    "                            if new_data[col].isnull().any())\n",
    "#makes a new column which has 1 if missing, and zero if not\n",
    "for col in missing_cols:\n",
    "    new_data[col + '_was_missing'] = new_data[col].isnull()*1\n",
    "\n",
    "#now the imputation part\n",
    "imputer = SimpleImputer()\n",
    "new_data_2 = pd.DataFrame(imputer.fit_transform(new_data))\n",
    "new_data_2.columns = new_data.columns\n",
    "\n",
    "#print(new_data_2.info())\n",
    "\n",
    "#now we dont have any missing values, lets make a function\n",
    "\n",
    "def impute_missing_values(df_train,df_test):\n",
    "    \"\"\"df and df_test should always come as a pair, to make sure that all data is imputed the same\"\"\"\n",
    "    #drop columns which should not be in the imputation\n",
    "    df_train = df_train.drop(columns=['price','area_kitchen','area_living','street','address'])\n",
    "    df_test = df_test.drop(columns=['area_kitchen','area_living','street','address'])\n",
    "\n",
    "    #Concat the two dataframes\n",
    "    df = pd.concat([df_train, df_test])\n",
    "\n",
    "    #find which columns is missing\n",
    "    missing_cols = (col for col in df.columns\n",
    "                            if df[col].isnull().any())\n",
    "\n",
    "    #add a was_missing column to all columns with missing data\n",
    "    for col in missing_cols:\n",
    "        df[col + '_was_missing'] = df[col].isnull()*1\n",
    "\n",
    "    #imputation\n",
    "    imputer = SimpleImputer()\n",
    "    df_2 = pd.DataFrame(imputer.fit_transform(df))\n",
    "    df_2.columns = df.columns\n",
    "\n",
    "    #now we need to split the two dataframes at the correct split\n",
    "    df_train = df_2.iloc[:df_train.shape[0]]\n",
    "    df_test_2 = df_2.iloc[-df_test.shape[0]:]\n",
    "    df_test = df_test_2.reset_index().drop(columns='index')\n",
    "\n",
    "    return df_train,df_test\n",
    "\n",
    "print(data.info())\n",
    "print(data_test.info())\n",
    "\n",
    "data_2, data_test_2 = impute_missing_values(data, data_test)\n",
    "\n",
    "print(data_2.info())\n",
    "print(data_test_2.info())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Clustering geodata**\n",
    "\n",
    "To further use the location of each house, we want to go further than district, anbd see if we can make another type of clustering.\n",
    "Will use k_means clustering with price as target attibute, lets see what we get\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters = 40, max_iter = 1000, init='k-means++')\n",
    "\n",
    "lat_long_pairs = data_train[['latitude','longitude']]\n",
    "target_data = np.log(data_train.price)/np.log(15)\n",
    "weighted_clusters = k_means.fit(lat_long_pairs,sample_weight = target_data)\n",
    "data_train['cluster_number'] = k_means.predict(lat_long_pairs, sample_weight = target_data)\n",
    "\n",
    "data_test = fix_geo_data(data_test)\n",
    "lat_long_pairs_test = data_test[['latitude','longitude']]\n",
    "print(lat_long_pairs_test.info())\n",
    "data_test['cluster_number'] = k_means.predict(lat_long_pairs_test)\n",
    "\n",
    "centers = k_means.cluster_centers_\n",
    "\n",
    "labels = data_train['cluster_number']\n",
    "labels_test = data_test['cluster_number']\n",
    "\n",
    "\n",
    "data_test.plot.scatter(x = 'latitude', y = 'longitude', c=labels_test, s=50, cmap='viridis')\n",
    "#data_train.plot.scatter(x = 'latitude', y = 'longitude', c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "plt.title('Clustering GPS Co-ordinates to Form Regions - Weighted',fontsize=18, fontweight='bold')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_train.plot.scatter(x = 'latitude', y = 'longitude', c=labels, s=50, cmap='viridis')\n",
    "#data_train.plot.scatter(x = 'latitude', y = 'longitude', c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "plt.title('Clustering GPS Co-ordinates to Form Regions - Weighted',fontsize=18, fontweight='bold')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Add trainstation cordinates**\n",
    "We want to check if there is any correleation between price and how close the apartment is to a railwaystation in moscow.\n",
    "We are also thinking about adding more centrum points so the models get a better sence of where things is located.\n",
    "\n",
    "Railwaystations: Kievsky, Paveletsky, Kursky, Belorussky, Rizhsky, Cherkizovo (Vostochniy), Yaroslavlsky, Kazansky, Leningradsky\n",
    "\n",
    "coordinates train sations:\n",
    "\n",
    "Want to add some other randomly selected points as well to represent the outer areas.\n",
    "\n",
    "best parks: Museum-Reserve Tsaritsyno, Vorobyovy Gory, MSU Botanical Garden Aptekarskiy Ogorod, Izmaylovsky Park\n",
    "\n",
    "water_treatment_proximity\n",
    "\n",
    "distance_to_top_tourist_spots(more attractive to rent?)\n",
    "\n",
    "distance_to_closest_metro_station: moscow is a city with a lot of traffic jams\n",
    "\n",
    "distance_to_closest_school:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  seller       price  area_total  area_kitchen  area_living  floor  \\\n",
      "0   0     3.0   7139520.0        59.2          12.5         31.0    2.0   \n",
      "1   1     NaN  10500000.0        88.0          14.2         48.0   18.0   \n",
      "2   2     3.0   9019650.0        78.5          22.5         40.8   12.0   \n",
      "3   3     NaN  10500000.0        88.0          14.0         48.0   18.0   \n",
      "4   4     NaN  13900000.0        78.0          17.0         35.0    7.0   \n",
      "\n",
      "   rooms  layout  ceiling  ...   radius6   radius7   radius8   radius9  \\\n",
      "0    2.0     NaN     2.65  ...  0.218782  0.171794  0.358689  0.320881   \n",
      "1    3.0     1.0      NaN  ...  0.242237  0.206450  0.123089  0.066118   \n",
      "2    3.0     NaN     2.65  ...  0.173617  0.046868  0.262504  0.208984   \n",
      "3    3.0     NaN      NaN  ...  0.242237  0.206450  0.123089  0.066118   \n",
      "4    2.0     1.0     2.90  ...  0.233716  0.140305  0.353153  0.304780   \n",
      "\n",
      "   radius10  radius11  radius12  radius13  radius14  radius15  \n",
      "0  0.321317  0.362697  0.373651  0.347781  0.281384  0.217403  \n",
      "1  0.078059  0.086922  0.182223  0.213989  0.197793  0.231701  \n",
      "2  0.196668  0.257661  0.292548  0.279911  0.167726  0.112869  \n",
      "3  0.078059  0.086922  0.182223  0.213989  0.197793  0.231701  \n",
      "4  0.289771  0.351692  0.376374  0.356488  0.235495  0.169864  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from moscow_housing.display_data import import_data\n",
    "\n",
    "data_train,data_test = import_data()\n",
    "def add_parks_and_schools(df):\n",
    "    \"\"\"with parks we are going to add all as radius to different parks, will also do this with schools\"\"\"\n",
    "    parks = pd.read_csv('parks_cordinates.csv')\n",
    "    schools = pd.read_csv('school_cordinates.csv')\n",
    "    parks = convert_from_coordinates_to_lat_long(parks)\n",
    "    schools = convert_from_coordinates_to_lat_long(schools)\n",
    "\n",
    "    total = remove_duplicates(parks,schools)\n",
    "\n",
    "    for i, cordinate in enumerate(total):\n",
    "        df['radius'+ str(i)] = np.sqrt((cordinate[0]-df['latitude'])**2 + (cordinate[1]-df['longitude'])**2)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def remove_duplicates(parks,schools):\n",
    "    \"\"\"we want to remove places which is really close\"\"\"\n",
    "    schools = schools.drop(schools[schools['school']=='????????????'].index)\n",
    "    schools = schools.drop(schools[schools['school']=='Electronics_and_maths'].index)\n",
    "    schools = schools.drop(schools[schools['school']=='MSU'].index)\n",
    "    parks = parks.drop(parks[parks['park']=='vorobyovy_gory'].index)\n",
    "    parks = parks.drop(parks[parks['park']=='bittsevskiy_park'].index)\n",
    "\n",
    "    return np.concatenate((schools[['latitude','longitude']].to_numpy(),parks[['latitude','longitude']].to_numpy()))\n",
    "\n",
    "def closest_park(df):\n",
    "    parks = pd.read_csv('parks_cordinates.csv')\n",
    "    parks = convert_from_coordinates_to_lat_long(parks)\n",
    "\n",
    "    df = find_closest(df,parks,'park')\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_school(df):\n",
    "    schools = pd.read_csv('school_cordinates.csv')\n",
    "    schools = convert_from_coordinates_to_lat_long(schools)\n",
    "\n",
    "    df = find_closest(df,schools,'school')\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_attraction(df):\n",
    "    attractions = pd.read_csv('attractions_cordinates.csv')\n",
    "    attractions = convert_from_coordinates_to_lat_long(attractions)\n",
    "\n",
    "    df = find_closest(df,attractions,'atraction')\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_attraction(df):\n",
    "    attractions = pd.read_csv('attractions_cordinates.csv')\n",
    "    attractions = convert_from_coordinates_to_lat_long(attractions)\n",
    "\n",
    "    df = find_closest(df,attractions,'atraction')\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_train(df):\n",
    "    trains = pd.read_csv('train_cordinates.csv')\n",
    "    trains = convert_from_coordinates_to_lat_long(trains)\n",
    "\n",
    "    df = find_closest(df,trains,'train_station')\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_metro(df):\n",
    "    metroes = pd.read_csv('metro_cordinates.csv')\n",
    "    metroes = convert_from_coordinates_to_lat_long(metroes,metro=True)\n",
    "\n",
    "    df = find_closest(df, metroes,'metro_station')\n",
    "\n",
    "    return df\n",
    "\n",
    "def find_closest(df,df_with_coordinates,text):\n",
    "    \"\"\"finds closets location for each location in df to all the locations in df_with_coordinates\"\"\"\n",
    "    df[['closest_' + text,'distance_' + text]] = df.apply(closest,args=(df_with_coordinates,text),axis=1)\n",
    "    return df\n",
    "\n",
    "def closest(df, list_of_coordinates_and_category,text):\n",
    "    if df.id%1000==0:\n",
    "        print(df.id)\n",
    "    \"\"\"finds the closets coordinate in list of coordinates and the given coordinate\"\"\"\n",
    "    point= df[['latitude','longitude']].to_numpy()\n",
    "    list_of_coordinates_and_category['distances'] = list_of_coordinates_and_category.apply(calculate_distance,args=(point[0],point[1]),axis=1)\n",
    "    shortest_row = list_of_coordinates_and_category['distances'].idxmin()\n",
    "    return list_of_coordinates_and_category.iloc[shortest_row][[text,'distances']]\n",
    "    #calculate_distance(coordinate, list_of_coordinates_and_category[['longitude','latitude']])\n",
    "\n",
    "def calculate_distance(df,pointlat,pointlong):\n",
    "    \"\"\"calculates the distance between two points\"\"\"\n",
    "    distance = np.sqrt((pointlat-df['latitude'])**2 + (pointlong-df['longitude'])**2)\n",
    "    return distance\n",
    "\n",
    "def convert_from_coordinates_to_lat_long(df,metro=False):\n",
    "    if metro:\n",
    "        df[['latitude','longitude']] = df['coordinates'].str.split(\" \",1, expand=True)\n",
    "        df[['latitude','longitude']] = df[['latitude','longitude']].astype(float)\n",
    "    else:\n",
    "        df[['latitude','longitude']] = df['coordinates'].str.split(',',1, expand=True)\n",
    "        df[['latitude','longitude']] = df[['latitude','longitude']].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "#data_train = closest_metro(data_train)\n",
    "#print(data_train.columns)\n",
    "#print(data_train[['distance_metro_station','closest_metro_station','latitude','longitude']].head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Stuck**\n",
    "\n",
    "Rigt now we are really stuck, but angle worked really well for centrum radius, so lets add some more angles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#make updatet add direction\n",
    "def add_direction(df, latitude=55.75, longitude=37.56,name=''):\n",
    "    \"\"\"adds direction to dataframe, can be one of eight categories (N,S,W,E)\"\"\"\n",
    "    #straight up (north)\n",
    "    normal_vector = np.array([0,1])\n",
    "    #normal_vector = np.tile(normal_vector,(df.shape[0],1))\n",
    "    #normal_vector = normal_vector.reshape((2,-df.shape[0]))\n",
    "    temp = pd.DataFrame()\n",
    "    temp['latitude'] = df['latitude']-latitude\n",
    "    temp['longitude'] = df['longitude']-longitude\n",
    "\n",
    "\n",
    "    apartment_vector = temp[['latitude','longitude']].to_numpy()\n",
    "\n",
    "\n",
    "    #print(np.shape(apartment_vector), np.shape(normal_vector))\n",
    "    angles = []\n",
    "    for vector in apartment_vector:\n",
    "        if vector[0] < 0:\n",
    "            temp_angle = -angle_between(vector,normal_vector)\n",
    "        else:\n",
    "            temp_angle = angle_between(vector,normal_vector)\n",
    "        angles.append(temp_angle)\n",
    "\n",
    "    angles = [element * 10 for element in angles]\n",
    "\n",
    "\n",
    "    angles_series = pd.Series(np.array(angles))\n",
    "    #angles_series.plot.hist()\n",
    "\n",
    "    df['angle'+name] = angles_series\n",
    "    max = df['angle'+name].max()\n",
    "    min = df['angle'+name].min()\n",
    "    bins = [min,min*7/8,min*5/8,min*3/8,min/8,max/8,max*3/8,max*5/8,max*7/8,max]\n",
    "    rounded_bins = [element for element in bins]\n",
    "    #print(rounded_bins)\n",
    "    direction = pd.cut(df['angle'+name], bins= rounded_bins,labels=['S','SW','W','NW','N','NE','E','SE','S'],ordered=False)\n",
    "    df['direction'+name] = direction\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **normalize data**\n",
    "we tried logging price, but maybe it is even better to normalize all integer data, or at least some of it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Styrk\\AppData\\Local\\Temp/ipykernel_22224/2775923982.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp[column] = (temp[column]-min_df)/(max_df-min_df)\n",
      "C:\\Users\\Styrk\\AppData\\Local\\Temp/ipykernel_22224/2775923982.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_test[column] = (temp_test[column]-min_df)/(max_df-min_df)\n"
     ]
    }
   ],
   "source": [
    "#here we can use a simple function, this function works pairwise\n",
    "def normalize_numerical(df,df_test, numerical_columns):\n",
    "    temp = df[numerical_columns]\n",
    "    temp_test = df_test[numerical_columns]\n",
    "    for column in numerical_columns:\n",
    "        min_df = min([temp[column].min(),temp_test[column].min()])\n",
    "        max_df = max([temp[column].max(),temp_test[column].max()])\n",
    "\n",
    "        temp[column] = (temp[column]-min_df)/(max_df-min_df)\n",
    "        temp_test[column] = (temp_test[column]-min_df)/(max_df-min_df)\n",
    "\n",
    "    df[numerical_columns] = temp\n",
    "    df_test[numerical_columns] = temp_test\n",
    "\n",
    "    return df,df_test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from moscow_housing.display_data import import_data\n",
    "\n",
    "data_train,data_test = import_data()\n",
    "\n",
    "\n",
    "num = ['ceiling']\n",
    "data_train,data_test = normalize_numerical(data_train,data_test,num)\n",
    "print(data_test.ceiling.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also saw some errores where floor were larger than stories, will fix this before adding high up:\n",
    "We think that high_up doesn't reward pent houses enough, meaning we want a new variable for penthouses\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Styrk\\AppData\\Local\\Temp/ipykernel_22224/3673722511.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['floor'][df['floor']>df['stories']] = df['stories']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, seller, price, area_total, area_kitchen, area_living, floor, rooms, layout, ceiling, bathrooms_shared, bathrooms_private, windows_court, windows_street, balconies, loggias, condition, phones, building_id, new, latitude, longitude, district, street, address, constructed, material, stories, elevator_without, elevator_passenger, elevator_service, parking, garbage_chute, heating]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 34 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'high_up'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3360\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3361\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3362\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'high_up'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22224/3673722511.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0madd_penthouse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'penthouse'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22224/3673722511.py\u001B[0m in \u001B[0;36madd_penthouse\u001B[1;34m(df)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0madd_penthouse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'penthouse'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'floor'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'high_up'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m-\u001B[0m \u001B[1;36m0.4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m**\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3456\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3457\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3458\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3459\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3460\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3361\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3362\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3363\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3364\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3365\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mis_scalar\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0misna\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhasnans\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'high_up'"
     ]
    }
   ],
   "source": [
    "def fix_floors(df):\n",
    "    df['floor'][df['floor']>df['stories']] = df['stories']\n",
    "    return df\n",
    "data_train,data_test = import_data()\n",
    "\n",
    "\n",
    "\n",
    "data_train = fix_floors(data_train)\n",
    "print(data_train[data_train['floor']>data_train['stories']])\n",
    "\n",
    "def add_penthouse(df):\n",
    "    \"\"\"will reward buildings highup in high buildings\"\"\"\n",
    "    df['penthouse'] = (df['floor'] * (df['high_up'] - 0.7)**3).astype(float)\n",
    "    return df\n",
    "\n",
    "print(add_penthouse(data_train)['penthouse'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# **Differences in test and training set**\n",
    "\n",
    "In this section i will try to look at the differences in the training and test set. Maybe there is some infomration which is really important in the training set, and not in the test set.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}