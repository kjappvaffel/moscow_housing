{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook I will test the model proposed in the ensemble learning lecture.\n",
    "Chapter with explenation about the algorithm behind the lightGBM, will write this after the coding.\n",
    "\n",
    "LightGBM is good in our case because it can deal with categorical values. We will start with importing the data, the data is already divided into test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I want to use import_data function from another directory, and make the data ready for the model. LightGBM accepts categorical values, but they have to be encoded as no-negative integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first run 0.1834119728836997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from moscow_housing.display_data import import_data\n",
    "\n",
    "#We dont want our model to care about the id of the house or the seller\n",
    "#In my first run, i will replace missing values with the mean value\n",
    "data, data_test = import_data()\n",
    "Y = data.price\n",
    "data = data.drop(columns=['price','id','seller'])\n",
    "\n",
    "\n",
    "for column in data.columns:\n",
    "    column_type = data[column].dtype\n",
    "    if column_type == 'object':\n",
    "        break\n",
    "    data[column] = data[column].replace(np.NaN, data[column].mean())\n",
    "\n",
    "#turn categorical features into correct type\n",
    "for column in data.columns:\n",
    "    column_type = data[column].dtype\n",
    "    if column_type == 'object':\n",
    "        data[column] = data[column].astype('category')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    assert (y_true >= 0).all()\n",
    "    assert (y_pred >= 0).all()\n",
    "    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n",
    "    return np.mean(log_error ** 2) ** 0.5\n",
    "\n",
    "\n",
    "lightGBM_model = lgb.LGBMRegressor(\n",
    "    num_leaves=20,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    silent=True,\n",
    "    metric='mse',\n",
    "    n_jobs=4,\n",
    "    n_estimators=10000,\n",
    "    colsample_bytree=0.95,\n",
    "    subsample=0.9,\n",
    "    learning_rate=0.09)\n",
    "\n",
    "lightGBM_model.fit(X_train,y_train)\n",
    "prediction = lightGBM_model.predict(X_test)\n",
    "\n",
    "count = 0\n",
    "for i in prediction:\n",
    "    if i < 0:\n",
    "        prediction[count] = prediction.mean()\n",
    "    count += 1\n",
    "\n",
    "rmsle = root_mean_squared_log_error(y_test,prediction)\n",
    "print(\"first run\", rmsle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Second attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object. You should provide an instance of scikit-learn estimator instead of a class.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8512/4223138469.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[0mimp_mean\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mIterativeImputer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mestimator\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mRandomForestRegressor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m \u001B[0mimp_mean\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m \u001B[0mtransformed_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimp_mean\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\impute\\_iterative.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    772\u001B[0m             \u001B[0mReturns\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    773\u001B[0m         \"\"\"\n\u001B[1;32m--> 774\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    775\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\impute\\_iterative.py\u001B[0m in \u001B[0;36mfit_transform\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    610\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_estimator\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBayesianRidge\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    611\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 612\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_estimator\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mclone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mestimator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    613\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    614\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimputation_sequence_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\u001B[0m in \u001B[0;36mclone\u001B[1;34m(estimator, safe)\u001B[0m\n\u001B[0;32m     63\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mestimator\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m                 raise TypeError(\n\u001B[0m\u001B[0;32m     66\u001B[0m                     \u001B[1;34m\"Cannot clone object. \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m                     \u001B[1;33m+\u001B[0m \u001B[1;34m\"You should provide an instance of \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: Cannot clone object. You should provide an instance of scikit-learn estimator instead of a class."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from moscow_housing.display_data import import_data\n",
    "\n",
    "#We dont want our model to care about the id of the house or the seller\n",
    "#For my second run, i want to use linear regression to predict the missing values\n",
    "data, data_test = import_data()\n",
    "data.drop(columns=['id', 'seller' ])\n",
    "\n",
    "#turn categorical features into correct type\n",
    "for column in data.columns:\n",
    "    column_type = data[column].dtype\n",
    "    if column_type == 'object':\n",
    "        print(column_type)\n",
    "        data[column] = data[column].astype('category')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "imp_mean = IterativeImputer(estimator=RandomForestRegressor, random_state=0)\n",
    "imp_mean.fit(data)\n",
    "transformed_data = imp_mean.transform(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = transformed_data.drop(columns='price')\n",
    "\n",
    "Y = data.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    assert (y_true >= 0).all()\n",
    "    assert (y_pred >= 0).all()\n",
    "    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n",
    "    return np.mean(log_error ** 2) ** 0.5\n",
    "\n",
    "\n",
    "lightGBM_model = lgb.LGBMRegressor(\n",
    "    num_leaves=20,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    silent=True,\n",
    "    metric='mse',\n",
    "    n_jobs=4,\n",
    "    n_estimators=10000,\n",
    "    colsample_bytree=0.95,\n",
    "    subsample=0.9,\n",
    "    learning_rate=0.09)\n",
    "\n",
    "lightGBM_model.fit(X_train,y_train)\n",
    "prediction = lightGBM_model.predict(X_test)\n",
    "\n",
    "count = 0\n",
    "for i in prediction:\n",
    "    if i < 0:\n",
    "        prediction[count] = prediction.mean()\n",
    "    count += 1\n",
    "\n",
    "rmsle = root_mean_squared_log_error(y_test,prediction)\n",
    "print(\"second run\", rmsle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}