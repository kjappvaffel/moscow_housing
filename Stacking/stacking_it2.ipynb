{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "plt.style.use('ggplot')\n",
    "#from feature_engineering import add_retning\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import sys\n",
    "import lightgbm as lgb\n",
    "import sklearn as skl\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "sys.path.append('../data')\n",
    "\n",
    "from display_data import import_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parks_and_schools(df):\n",
    "    \"\"\"with parks we are going to add all as radius to different parks, will also do this with schools\"\"\"\n",
    "    parks = pd.read_csv('parks_cordinates.csv')\n",
    "    schools = pd.read_csv('school_cordinates.csv')\n",
    "    parks = convert_from_coordinates_to_lat_long(parks)\n",
    "    schools = convert_from_coordinates_to_lat_long(schools)\n",
    "\n",
    "    total = remove_duplicates(parks,schools)\n",
    "\n",
    "    for i, cordinate in enumerate(total):\n",
    "        df['radius'+ str(i)] = np.sqrt((cordinate[0]-df['latitude'])**2 + (cordinate[1]-df['longitude'])**2)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def remove_duplicates(parks,schools):\n",
    "    \"\"\"we want to remove places which is really close\"\"\"\n",
    "    schools = schools.drop(schools[schools['school']=='????????????'].index)\n",
    "    schools = schools.drop(schools[schools['school']=='Electronics_and_maths'].index)\n",
    "    schools = schools.drop(schools[schools['school']=='MSU'].index)\n",
    "    parks = parks.drop(parks[parks['park']=='vorobyovy_gory'].index)\n",
    "    parks = parks.drop(parks[parks['park']=='bittsevskiy_park'].index)\n",
    "\n",
    "    return np.concatenate((schools[['latitude','longitude']].to_numpy(),parks[['latitude','longitude']].to_numpy()))\n",
    "\n",
    "def closest_park(df):\n",
    "    parks = pd.read_csv('parks_cordinates.csv')\n",
    "    parks = convert_from_coordinates_to_lat_long(parks)\n",
    "\n",
    "    df = find_closest(df,parks,'park')\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_school(df):\n",
    "    schools = pd.read_csv('school_cordinates.csv')\n",
    "    schools = convert_from_coordinates_to_lat_long(schools)\n",
    "\n",
    "    df = find_closest(df,schools,'school')\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_attraction(df):\n",
    "    attractions = pd.read_csv('attractions_cordinates.csv')\n",
    "    attractions = convert_from_coordinates_to_lat_long(attractions)\n",
    "\n",
    "    df = find_closest(df,attractions,'atraction')\n",
    "\n",
    "    return df\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_school(df):\n",
    "    schools = pd.read_csv('school_cordinates.csv')\n",
    "    schools = convert_from_coordinates_to_lat_long(schools)\n",
    "\n",
    "    df = find_closest(df,schools,'school')\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_attraction(df):\n",
    "    attractions = pd.read_csv('attractions_cordinates.csv')\n",
    "    attractions = convert_from_coordinates_to_lat_long(attractions)\n",
    "\n",
    "    df = find_closest(df,attractions,'atraction')\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_attraction(df):\n",
    "    attractions = pd.read_csv('attractions_cordinates.csv')\n",
    "    attractions = convert_from_coordinates_to_lat_long(attractions)\n",
    "\n",
    "    df = find_closest(df,attractions,'atraction')\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_train(df):\n",
    "    trains = pd.read_csv('train_cordinates.csv')\n",
    "    trains = convert_from_coordinates_to_lat_long(trains)\n",
    "\n",
    "    df = find_closest(df,trains,'train_station')\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest_metro(df):\n",
    "    metroes = pd.read_csv('metro_cordinates.csv')\n",
    "    metroes = convert_from_coordinates_to_lat_long(metroes,metro=True)\n",
    "\n",
    "    df = find_closest(df, metroes,'metro_station')\n",
    "\n",
    "    return df\n",
    "\n",
    "def find_closest(df,df_with_coordinates,text):\n",
    "    \"\"\"finds closets location for each location in df to all the locations in df_with_coordinates\"\"\"\n",
    "    df[['closest_' + text,'distance_' + text]] = df.apply(closest,args=(df_with_coordinates,text),axis=1)\n",
    "    df['closest_'+text] = df['closest_'+text].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def closest(df, list_of_coordinates_and_category,text):\n",
    "    if df.id%1000==0:\n",
    "        print(df.id)\n",
    "    \"\"\"finds the closets coordinate in list of coordinates and the given coordinate\"\"\"\n",
    "    point= df[['latitude','longitude']].to_numpy()\n",
    "    list_of_coordinates_and_category['distances'] = list_of_coordinates_and_category.apply(calculate_distance,args=(point[0],point[1]),axis=1)\n",
    "    shortest_row = list_of_coordinates_and_category['distances'].idxmin()\n",
    "    return list_of_coordinates_and_category.iloc[shortest_row][[text,'distances']]\n",
    "    #calculate_distance(coordinate, list_of_coordinates_and_category[['longitude','latitude']])\n",
    "\n",
    "def calculate_distance(df,pointlat,pointlong):\n",
    "    \"\"\"calculates the distance between two points\"\"\"\n",
    "    distance = np.sqrt((pointlat-df['latitude'])**2 + (pointlong-df['longitude'])**2)\n",
    "    return distance\n",
    "\n",
    "def convert_from_coordinates_to_lat_long(df,metro=False):\n",
    "    if metro:\n",
    "        df[['latitude','longitude']] = df['coordinates'].str.split(\" \",1, expand=True)\n",
    "        df[['latitude','longitude']] = df[['latitude','longitude']].astype(float)\n",
    "    else:\n",
    "        df[['latitude','longitude']] = df['coordinates'].str.split(',',1, expand=True)\n",
    "        df[['latitude','longitude']] = df[['latitude','longitude']].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def fix_missing_value(df):\n",
    "    #layout, set everythin to 1\n",
    "    df['layout'].fillna(int(1.0),inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    #found correleation between number of stories and balconies,loggias\n",
    "    df.loc[(df['stories'] < 7.0)&df['loggias'].isna(),'loggias'] = 0\n",
    "    df.loc[(df['stories'] < 7.0)&df['balconies'].isna(),'balconies'] = 1\n",
    "\n",
    "    df.loc[((df['stories'] > 12) & (df['stories'] < 16))|((df['stories'] > 17) & (df['stories'] < 24))&(df['loggias'].isna()),'loggias'] = 1\n",
    "    df.loc[((df['stories'] > 12) & (df['stories'] < 16))|((df['stories'] > 17) & (df['stories'] < 24))&(df['loggias'].isna()),'balconies'] = 0\n",
    "\n",
    "\n",
    "    #use the correleation between balconies and loggias to replace nans,\n",
    "    df.loc[(df['loggias'] == 0.0 )& df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[(df['balconies'] == 0.0 )& df['loggias'].isna(),'loggias'] = 1\n",
    "    df.loc[(df['loggias'] > 0.0 )& df['balconies'].isna(),'balconies'] = 0\n",
    "    df.loc[(df['balconies'] > 0.0 )& df['loggias'].isna(),'loggias'] = 0\n",
    "\n",
    "    #use correlation between material and ceiling groups\n",
    "    ceiling_under_2 = len(df[((df['material'] == 4)|(df['material'] == 3)) & df['ceiling'].isna()])\n",
    "    sample = df[df['ceiling'] < 2.6]['ceiling'].sample(n=ceiling_under_2,replace=True).to_numpy()\n",
    "    df.loc[((df['material'] == 4)|(df['material'] == 3)) & (df['ceiling'].isna()),'ceiling'] = sample\n",
    "    #use corelation between condition and ceiling groups\n",
    "    ceiling_under_2 = len(df[(df['condition'] == 1) & df['ceiling'].isna()])\n",
    "    sample = df[df['ceiling'] < 2.6]['ceiling'].sample(n=ceiling_under_2,replace=True).to_numpy()\n",
    "    df.loc[(df['condition'] == 1) & df['ceiling'].isna(),'ceiling'] = sample\n",
    "\n",
    "\n",
    "    #use correleation betwwen ceiling and area\n",
    "    ceiling_over_3 = len(df[(df['area_total'] > 104)&(df['ceiling'].isna())])\n",
    "    sample = df[df['ceiling'] > 3.0]['ceiling'].sample(n=ceiling_over_3,replace=True).to_numpy()\n",
    "    df.loc[(df['area_total'] > 104)&(df['ceiling'].isna() ),'ceiling'] = sample\n",
    "\n",
    "    ceiling_over_3 = len(df[(df['area_total'] > 75)&(df['ceiling'].isna())])\n",
    "    sample = df[df['ceiling'] > 2.95]['ceiling'].sample(n=ceiling_over_3,replace=True).to_numpy()\n",
    "    df.loc[(df['area_total'] > 75)&(df['ceiling'].isna() ),'ceiling'] = sample\n",
    "\n",
    "    ceiling_under_2 = len(df[(df['area_total'] < 55)&(df['ceiling'].isna())])\n",
    "    sample = df[df['ceiling'] < 2.8]['ceiling'].sample(n=ceiling_under_2,replace=True).to_numpy()\n",
    "    df.loc[(df['area_total'] < 55)&(df['ceiling'].isna() ),'ceiling'] = sample\n",
    "\n",
    "    #seller\n",
    "    #use correleation between seller and new\n",
    "    df.loc[(df['new'] == 1 )& df['seller'].isna(),'seller'] = 3\n",
    "\n",
    "    #use correleation between seller and constructed\n",
    "    df.loc[(df['constructed'] > 2018)& df['seller'].isna(),'seller'] = 3\n",
    "    df.loc[(df['constructed'] < 1990)& df['seller'].isna(),'seller'] = 1\n",
    "\n",
    "    #use correleation between seller and stories\n",
    "    df.loc[(df['stories'] > 17)& df['seller'].isna(),'seller'] = 3\n",
    "    df.loc[(df['stories'] < 9)& df['seller'].isna(),'seller'] = 1\n",
    "\n",
    "    #use correleation between seller and district\n",
    "    df.loc[((df['district'] == 3)|(df['district']==6))& df['seller'].isna(),'seller'] = 1\n",
    "    df.loc[((df['district'] == 11)|(df['district']==2))& df['seller'].isna(),'seller'] = 3\n",
    "\n",
    "    #conditon\n",
    "    #use correleation between condition and new\n",
    "    df.loc[(df['new'] == 1)& df['condition'].isna(),'condition'] = 0\n",
    "\n",
    "    #use correleation between condition and constructed year\n",
    "    df.loc[(df['constructed'] >= 2017)& df['condition'].isna(),'condition'] = 0\n",
    "    df.loc[(df['constructed'] <= 1989)& df['condition'].isna(),'condition'] = 1\n",
    "\n",
    "    #material\n",
    "    #use correlation between material and district\n",
    "    df.loc[(df['district'] == 3)& df['material'].isna(),'material'] = 3\n",
    "\n",
    "    #use correleation between material and constructed year\n",
    "    df.loc[((df['constructed'] > 1970)& (df['constructed'] <= 1989))&df['material'].isna(),'material'] = 3\n",
    "    df.loc[(df['constructed'] <= 1970)&df['material'].isna(),'material'] = 0\n",
    "\n",
    "    #use correleation between stories and material\n",
    "    df.loc[(df['stories'] < 7)& df['material'].isna(),'material'] = 0\n",
    "    df.loc[((df['stories'] > 7)&(df['stories'] < 9))& df['material'].isna(),'material'] = 3\n",
    "\n",
    "    #use correleation between conditon and material\n",
    "    df.loc[(df['condition'] == 3)& df['material'].isna(),'material'] = 3\n",
    "    df.loc[(df['condition'] == 0)& df['material'].isna(),'material'] = 2\n",
    "\n",
    "    #use correleation between parking and material\n",
    "    df.loc[((df['parking'] == 3)|(df['parking'] == 2))& df['material'].isna(),'material'] = 2\n",
    "\n",
    "    #user correlation between area and material\n",
    "    df.loc[(df['area_total'] > 104)& df['material'].isna(),'material'] = 2\n",
    "\n",
    "    #parking\n",
    "    #use coreletion between parking and material\n",
    "    df.loc[((df['material'] == 0)|(df['material'] == 3))& df['parking'].isna(),'parking'] = 1\n",
    "\n",
    "    #use correleation between stories and parking\n",
    "    df.loc[((df['stories'] > 30))& df['parking'].isna(),'parking'] = 0\n",
    "    df.loc[((df['stories'] > 7)&(df['stories'] < 9))& df['parking'].isna(),'parking'] = 1\n",
    "\n",
    "    #use correleation between radius and parking\n",
    "    df.loc[(df['radius'] < 0.0547)& df['parking'].isna(),'parking'] = 0\n",
    "    df.loc[(df['radius'] > 0.251)& df['parking'].isna(),'parking'] = 1\n",
    "\n",
    "    #windows_court and windows_street\n",
    "    #windows_street and balconies\n",
    "    df.loc[(df['balconies'] == 2)& df['windows_street'].isna(),'windows_street'] = 1\n",
    "\n",
    "    #windows street and area\n",
    "    df.loc[(df['area_total'] > 106)& df['windows_street'].isna(),'windows_street'] = 1\n",
    "    df.loc[(df['area_total'] < 47)& df['windows_street'].isna(),'windows_street'] = 0\n",
    "\n",
    "    #windwos_street and stories\n",
    "    df.loc[(df['stories'] > 40)& df['windows_street'].isna(),'windows_street'] = 1\n",
    "    df.loc[(df['stories'] < 9)& df['windows_street'].isna(),'windows_street'] = 0\n",
    "\n",
    "    #windows_street and windows_court\n",
    "    df.loc[(df['windows_street'] == 0)& df['windows_court'].isna(),'windows_court'] = 1\n",
    "\n",
    "    df.loc[((df['windows_street'] == 1)& (df['area_total'] > 75)) & df['windows_court'].isna(),'windows_court'] = 1\n",
    "    df.loc[((df['windows_street'] == 1)& (df['area_total'] < 47)) & df['windows_court'].isna(),'windows_court'] = 0\n",
    "\n",
    "    #balconies\n",
    "    #balconies and stories\n",
    "    df.loc[((df['stories'] >= 12)& (df['area_total'] <= 16)) & df['balconies'].isna(),'balconies'] = 0\n",
    "    df.loc[(df['stories'] < 7) & df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[((df['stories'] > 17)& (df['area_total'] <= 24)) & df['balconies'].isna(),'balconies'] = 0\n",
    "\n",
    "    #matrial and balconies\n",
    "    df.loc[(df['material'] == 0) & df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[(df['material'] == 2) & df['balconies'].isna(),'balconies'] = 0\n",
    "\n",
    "    #balconies and constructed\n",
    "    df.loc[(df['constructed'] < 1970) & df['balconies'].isna(),'balconies'] = 1\n",
    "    df.loc[(df['constructed'] > 2018) & df['balconies'].isna(),'balconies'] = 0\n",
    "\n",
    "    #loggias\n",
    "    #loggias and balconies\n",
    "    df.loc[(df['balconies'] == 0) & df['loggias'].isna(),'loggias'] = 1\n",
    "    df.loc[(df['balconies'] > 0) & df['loggias'].isna(),'loggias'] = 0\n",
    "\n",
    "    #loggias and stories\n",
    "    df.loc[(df['stories'] < 7) & df['loggias'].isna(),'loggias'] = 0\n",
    "    df.loc[((df['stories'] >= 17)& (df['area_total'] <= 24)) & df['loggias'].isna(),'loggias'] = 1\n",
    "\n",
    "    #bathrooms shared, bathroomes private\n",
    "\n",
    "    #loggias and blaconies\n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_radius(df):\n",
    "    #adds radius column to dataframe\n",
    "    df['radius'] = np.sqrt((df['latitude']-55.75)**2 + (df['longitude']-37.55)**2)\n",
    "    return df\n",
    "\n",
    "def group_by_feature_and_price(df, feature, data_train = pd.DataFrame()):\n",
    "    #makes a new column with the mean price for each group to all rows\n",
    "    if data_train.empty:\n",
    "        grouped = df.groupby([feature])\n",
    "\n",
    "        mean_price = grouped['price'].mean()\n",
    "\n",
    "        df_merged = pd.merge(df,mean_price, on=feature, how='left')\n",
    "\n",
    "        column_name = str(feature + '_price')\n",
    "\n",
    "        df[column_name] = df_merged['price_y']\n",
    "    else:\n",
    "        grouped = data_train.groupby([feature])\n",
    "\n",
    "        mean_price = grouped['price'].mean()\n",
    "\n",
    "        df_merged = pd.merge(df,mean_price, on=feature, how='left')\n",
    "\n",
    "        column_name = str(feature + '_price')\n",
    "\n",
    "        df[column_name] = df_merged['price']\n",
    "    return df\n",
    "\n",
    "def one_hot_encode(df, column_to_encode):\n",
    "    \"\"\"one hots encode for one singel column\"\"\"\n",
    "    encoded_df = pd.get_dummies(df[[column_to_encode]].astype(str))\n",
    "    df = pd.concat([df,encoded_df],axis=1)\n",
    "    return df\n",
    "\n",
    "def one_hot_encode_multiple(df, list_of_columns):\n",
    "    \"\"\"takes in multiple columns and runs one hot encode for each column\"\"\"\n",
    "    for column_to_encode in list_of_columns:\n",
    "        #print(column_to_encode)\n",
    "        df = one_hot_encode(df, column_to_encode)\n",
    "    return df\n",
    "\n",
    "def add_direction(df):\n",
    "    \"\"\"adds direction to dataframe, can be one of eight categories (N,S,W,E)\"\"\"\n",
    "    #straight up (north)\n",
    "    normal_vector = np.array([0,1])\n",
    "    #normal_vector = np.tile(normal_vector,(df.shape[0],1))\n",
    "    #normal_vector = normal_vector.reshape((2,-df.shape[0]))\n",
    "    temp = pd.DataFrame()\n",
    "    temp['latitude'] = df['latitude']-55.75\n",
    "    temp['longitude'] = df['longitude']-37.56\n",
    "\n",
    "\n",
    "    apartment_vector = temp[['latitude','longitude']].to_numpy()\n",
    "\n",
    "\n",
    "    #print(np.shape(apartment_vector), np.shape(normal_vector))\n",
    "    angles = []\n",
    "    for vector in apartment_vector:\n",
    "        if vector[0] < 0:\n",
    "            temp_angle = -angle_between(vector,normal_vector)\n",
    "        else:\n",
    "            temp_angle = angle_between(vector,normal_vector)\n",
    "        angles.append(temp_angle)\n",
    "\n",
    "    angles = [element * 10 for element in angles]\n",
    "\n",
    "\n",
    "    angles_series = pd.Series(np.array(angles))\n",
    "    #angles_series.plot.hist()\n",
    "\n",
    "    df['angle'] = angles_series\n",
    "    max = df.angle.max()\n",
    "    min = df.angle.min()\n",
    "    bins = [min,min*7/8,min*5/8,min*3/8,min/8,max/8,max*3/8,max*5/8,max*7/8,max]\n",
    "    rounded_bins = [element for element in bins]\n",
    "    #print(rounded_bins)\n",
    "    direction = pd.cut(df.angle, bins= rounded_bins,labels=['S','SW','W','NW','N','NE','E','SE','S'],ordered=False)\n",
    "    df['direction'] = direction\n",
    "    return df\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\"returns the unit vector if the vector\"\"\"\n",
    "    return vector/np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1,v2):\n",
    "    \"\"\"returns angle between two vectors in radian\"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u,v2_u),-1,1))\n",
    "\n",
    "def fix_geo_data(data_test):\n",
    "    \"\"\"should just be called on test data\"\"\"\n",
    "    #fix all radius issues\n",
    "    data_test._set_value(23,'longitude',37.473761)\n",
    "    data_test._set_value(23,'latitude',55.560891)\n",
    "    data_test._set_value(90,'longitude',37.473761)\n",
    "    data_test._set_value(90,'latitude',55.560891)\n",
    "\n",
    "    data_test._set_value(2511,'longitude',37.478055)\n",
    "    data_test._set_value(2511,'latitude',55.544046)\n",
    "    data_test._set_value(5090,'longitude',37.478055)\n",
    "    data_test._set_value(5090,'latitude',55.544046)\n",
    "    data_test._set_value(6959,'longitude',37.478055)\n",
    "    data_test._set_value(6959,'latitude',55.544046)\n",
    "    data_test._set_value(8596,'longitude',37.478055)\n",
    "    data_test._set_value(8596,'latitude',55.544046)\n",
    "\n",
    "    data_test._set_value(4719,'longitude',37.385493)\n",
    "    data_test._set_value(4719,'latitude',55.853117)\n",
    "\n",
    "    data_test._set_value(9547,'longitude',37.384711)\n",
    "    data_test._set_value(9547,'latitude',55.853511)\n",
    "\n",
    "    data_test._set_value(2529,'longitude',37.464994)\n",
    "    data_test._set_value(2529,'latitude',55.627666)\n",
    "\n",
    "    data_test = add_radius(data_test)\n",
    "\n",
    "    return data_test\n",
    "\n",
    "def drop_n_largest(data_train):\n",
    "    \"\"\"drops 4 largest values, should only be called on training set\"\"\"\n",
    "    #drop 4 largest from training data, maybe not samrt, but we will see\n",
    "    data_train.drop([3217,21414,15840,13938])\n",
    "    return data_train\n",
    "\n",
    "def clean_data(all_data):\n",
    "    \"\"\"cleans the data with all the knowledge we have so far\"\"\"\n",
    "\n",
    "\n",
    "    #fix ceiling issues\n",
    "    all_data.loc[all_data['ceiling'] > 50,'ceiling']*=0.01\n",
    "    all_data.loc[all_data['ceiling'] > 25, 'ceiling']*=0.1\n",
    "    all_data.loc[all_data['ceiling'] < 0.5,'ceiling'] = float('NaN')\n",
    "    \"\"\"\"\n",
    "    #fix area_kitchen and area_living issues\n",
    "    all_data['living'] = all_data.area_living/all_data.area_total\n",
    "    all_data['kitchen'] = all_data.area_total/all_data.area_kitchen\n",
    "\n",
    "    all_data.loc[all_data['living'] > 1,'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['living'] > 1,'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "\n",
    "    all_data.loc[all_data.area_living/all_data.area_total > 1, 'area_living'] = float('NaN')\n",
    "\n",
    "    all_data['living'] = all_data.area_living/all_data.area_total\n",
    "    all_data['kitchen'] = all_data.area_kitchen/all_data.area_total\n",
    "\n",
    "    all_data['sum_area'] = all_data.area_living + all_data.area_kitchen\n",
    "    all_data.loc[all_data['sum_area'] == 100, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 100, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "\n",
    "    #this fixed some few rows.\n",
    "    #when printing we see some other very clear \"precentage situations, fixing these\n",
    "    all_data.loc[all_data['sum_area'] == 38.5, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 38.5, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 52.7, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 52.7, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 71.6, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 71.6, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 20), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 20), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 15), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 15), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 10), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 10), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 30), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 30), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 10), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 10), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 15), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 15), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 20), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 20), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 25), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 25), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 30), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 30), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 50), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 50), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 60) & (all_data['area_total'] > 120), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 60) & (all_data['area_total'] > 120), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] < 70) & (all_data['area_total'] > 120)&(all_data['rooms'] < 3), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] < 70) & (all_data['area_total'] > 120)&(all_data['rooms'] < 3), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    \"\"\"\n",
    "    return all_data\n",
    "\n",
    "def add_high_up(df):\n",
    "    \"\"\"add exponetital function to determine how high up a building is\"\"\"\n",
    "    high_up = df.floor/df.stories\n",
    "    high_up_exp = np.exp(high_up) - 1\n",
    "    euler = np.exp(1)\n",
    "\n",
    "    df['high_up'] = high_up_exp\n",
    "    df['high_up'].where(df['high_up'] > euler, euler)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def log_radius(df):\n",
    "    df['radius'] = np.log(df['radius'])\n",
    "    return df\n",
    "\n",
    "def cluster_geo_data(df,df_test):\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "    k_means = KMeans(n_clusters = 100, max_iter = 1000, init='k-means++')\n",
    "\n",
    "    lat_long_pairs = df[['latitude','longitude']]\n",
    "    lat_long_pairs_test = df_test[['latitude','longitude']]\n",
    "    target_data = np.log(df.price)/np.log(15)\n",
    "\n",
    "    k_means.fit(lat_long_pairs,sample_weight = target_data)\n",
    "    df['cluster_number'] = k_means.predict(lat_long_pairs)\n",
    "    df_test['cluster_number'] = k_means.predict(lat_long_pairs_test)\n",
    "\n",
    "    return df, df_test\n",
    "\n",
    "def fix_radius(data_test):\n",
    "    \"\"\"should just be called on test data\"\"\"\n",
    "    #fix all radius issues\n",
    "    data_test._set_value(23,'radius',0.203899)\n",
    "    data_test._set_value(90,'radius',0.203899)\n",
    "    data_test._set_value(2511,'radius',0.218159)\n",
    "    data_test._set_value(5090,'radius',0.218159)\n",
    "    data_test._set_value(6959,'radius',0.218159)\n",
    "    data_test._set_value(8596,'radius',0.218159)\n",
    "    data_test._set_value(4719, 'radius',0.19580)\n",
    "    data_test._set_value(9547, 'radius',0.19520)\n",
    "    data_test._set_value(2529, 'radius', np.sqrt((37.464994-37.55)**2+(55.627666-55.75)**2))\n",
    "\n",
    "    return data_test    \n",
    "\n",
    "def rf_data_prep(data_rf,data_test_rf):\n",
    "    Y = data_rf.price\n",
    "    radius = True\n",
    "    # Add radius\n",
    "    if(radius):\n",
    "        data_rf = add_radius(data_rf)\n",
    "        data_test = add_radius(data_test_rf)\n",
    "        #coordinates = ['latitude', 'longitude']\n",
    "        #data = data.drop(columns = coordinates)\n",
    "        #data_test = data_test.drop(columns = coordinates)\n",
    "\n",
    "\n",
    "    data_rf = data_rf.drop(columns = ['area_living', 'area_kitchen'])\n",
    "    data_test_rf = data_test_rf.drop(columns = ['area_living', 'area_kitchen'])\n",
    "\n",
    "    categorical_data = ['seller','layout', 'windows_court', 'windows_street', 'condition', 'building_id','new','district','street',\n",
    "                        'address', 'material', 'elevator_without', 'elevator_passenger', 'elevator_service', 'parking','garbage_chute', 'heating']\n",
    "    cleaning = True\n",
    "\n",
    "    if(cleaning):\n",
    "        data_test_rf = fix_radius(data_test_rf)\n",
    "        data_rf = clean_data(data_rf)\n",
    "        data_test_rf = clean_data(data_test_rf)\n",
    "\n",
    "    data_columns_rf = list(data_rf.columns)\n",
    "    numerical_data = list(set(data_columns_rf)-set(categorical_data))\n",
    "    print(numerical_data)\n",
    "    for column in numerical_data:\n",
    "        mean = data_rf[column].mean()\n",
    "        data_rf[column] = data_rf[column].replace(np.NaN, mean)\n",
    "        if column != 'price':\n",
    "            mean_test = data_test_rf[column].mean()\n",
    "            data_test_rf[column] = data_test_rf[column].replace(np.NAN,mean)\n",
    "\n",
    "    #Features\n",
    "    radius = True\n",
    "    district_mean_price = False #her er det nan\n",
    "    ohe = True\n",
    "    direction = True\n",
    "    high_up = True\n",
    "    #add high up\n",
    "\n",
    "\n",
    "    if(high_up):\n",
    "        data_rf = add_high_up(data_rf)\n",
    "        data_test_rf = add_high_up(data_test_rf)\n",
    "\n",
    "    if(district_mean_price):\n",
    "        data_rf = group_by_feature_and_price(data_rf,'district')\n",
    "        data_test_rf = group_by_feature_and_price(data_test_rf,'district',data_train=data_rf)\n",
    "        data_rf = data_rf.drop(columns=['price', 'id'])\n",
    "\n",
    "\n",
    "    if(direction):\n",
    "        data_rf = add_direction(data_rf)\n",
    "        data_test_rf = add_direction(data_test_rf)\n",
    "        data_rf = one_hot_encode(data_rf, 'direction')\n",
    "        data_test_rf = one_hot_encode(data_test_rf, 'direction')\n",
    "        data_rf = data_rf.drop(columns = 'direction')\n",
    "        data_test_rf = data_test_rf.drop(columns = 'direction')\n",
    "\n",
    "\n",
    "    #One hot encode data\n",
    "    if(ohe):\n",
    "        encode_categorical_data_rf = ['seller','layout', 'windows_court', 'windows_street', 'condition','new','district',\n",
    "                        'material', 'elevator_without', 'elevator_passenger', 'elevator_service', 'parking','garbage_chute', 'heating']\n",
    "        data_rf = one_hot_encode_multiple(data_rf,encode_categorical_data_rf)\n",
    "        data_test_rf = one_hot_encode_multiple(data_test_rf,encode_categorical_data_rf)\n",
    "    #####HER MÅ DET FIKSES \n",
    "    # skal man logtransformere`?\n",
    "    #data_rf['price'] = np.log(data_rf['price'])/np.log(15)\n",
    "    ### TROR IKKE MAN TRENGER SCALE\n",
    "    #scaler = MinMaxScaler() # mapper alt til mellom 0 og 1, default\n",
    "    #data_rf[numerical_data] = scaler.fit_transform(data_rf[numerical_data])\n",
    "    #Y = scale(Y) ##FIKS her\n",
    "    #Y = data_rf['price']\n",
    "    data_rf = data_rf.drop(columns=['price', 'id'])\n",
    "\n",
    "    #Drop cat_data\n",
    "    data_rf = data_rf.drop(columns = categorical_data) # har one-hot encoda lengre oppe\n",
    "    data_test_rf = data_test_rf.drop(columns=['id'])\n",
    "    data_test_rf = data_test_rf.drop(columns = categorical_data) #må huske testdataen\n",
    "    \n",
    "    ## Rot under her\n",
    "\n",
    "    nan_values = data_rf.isna().any()\n",
    "    nan_columns = nan_values.any()\n",
    "\n",
    "\n",
    "    columns_with_nan = data_rf.columns[nan_columns].tolist()\n",
    "    #print(nan_values)\n",
    "    #print(columns_with_nan)\n",
    "    for column in data_rf.columns:\n",
    "        if data_rf[column].isna().any():\n",
    "            print(column)\n",
    "\n",
    "    print(Y)\n",
    "    if (Y.values < 0).any():\n",
    "        print('Gunnar')\n",
    "    return data_rf, data_test_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['longitude', 'area_living', 'latitude', 'phones', 'area_kitchen', 'bathrooms_shared', 'rooms', 'bathrooms_private', 'id', 'ceiling', 'constructed', 'area_total', 'loggias', 'radius', 'stories', 'balconies', 'floor', 'price']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23285 entries, 0 to 23284\n",
      "Columns: 189 entries, area_total to cluster_number_99\n",
      "dtypes: float64(15), uint8(174)\n",
      "memory usage: 6.5 MB\n",
      "None\n",
      "['longitude', 'latitude', 'phones', 'bathrooms_shared', 'rooms', 'bathrooms_private', 'id', 'ceiling', 'constructed', 'area_total', 'loggias', 'radius', 'stories', 'balconies', 'floor', 'price']\n",
      "0         7139520.0\n",
      "1        10500000.0\n",
      "2         9019650.0\n",
      "3        10500000.0\n",
      "4        13900000.0\n",
      "            ...    \n",
      "23280    13300000.0\n",
      "23281    15854300.0\n",
      "23282    19800000.0\n",
      "23283    29999000.0\n",
      "23284    10950000.0\n",
      "Name: price, Length: 23285, dtype: float64\n",
      "done here\n"
     ]
    }
   ],
   "source": [
    "#We dont want our model to care about the id of the house or the seller\n",
    "data, data_test = import_data()\n",
    "data_light, data_test_light = import_data()\n",
    "\n",
    "Y = data.price\n",
    "test_id = data_test.id\n",
    "\n",
    "radius = True\n",
    "# Add radius\n",
    "if(radius):\n",
    "    data = add_radius(data)\n",
    "    data_test = add_radius(data_test)\n",
    "    #coordinates = ['latitude', 'longitude']\n",
    "    #data = data.drop(columns = coordinates)\n",
    "    #data_test = data_test.drop(columns = coordinates)\n",
    "    #data_light = add_radius(data_light)\n",
    "    #data_test_light = add_radius(data_test_light)\n",
    "\n",
    "fix_missing = False\n",
    "if fix_missing:\n",
    "    data = fix_missing_value(data)\n",
    "    data_test = fix_missing_value(data_test)\n",
    "    #data_light = fix_missing_value(data_light)\n",
    "    #data_test_light = fix_missing_value(data_test_light)\n",
    "\n",
    "cleaning = True\n",
    "if(cleaning):\n",
    "    data_test = fix_geo_data(data_test)\n",
    "    data = clean_data(data)\n",
    "    data_test = clean_data(data_test)\n",
    "    #data_test_light = fix_geo_data(data_test_light)\n",
    "    #data_light = clean_data(data_light)\n",
    "    #data_test_light = clean_data(data_test_light)\n",
    "\n",
    "closest_school = False\n",
    "closest_park = False\n",
    "closest_metro = False\n",
    "closest_attraction = False\n",
    "closest_train = False\n",
    "\"\"\"when setting one of these to True, you need to make sure to add the different important things.\"\"\"\n",
    "\n",
    "if (closest_school):\n",
    "    data = closest_school(data)\n",
    "    data_test = closest_school(data_test)\n",
    "\n",
    "if (closest_park):\n",
    "    data = closest_park(data)\n",
    "    data_test = closest_park(data_test)\n",
    "\n",
    "if (closest_train):\n",
    "    data = closest_train(data)\n",
    "    data_test = closest_train(data_test)\n",
    "\n",
    "if (closest_metro):\n",
    "    data = closest_metro(data)\n",
    "    data_test = closest_metro(data_test)\n",
    "\n",
    "if (closest_attraction):\n",
    "    data = closest_attraction(data)\n",
    "    data_test = closest_attraction(data_test)\n",
    "\n",
    "\n",
    "data_test_light = data_test\n",
    "data_light = data\n",
    "categorical_data = ['seller','layout', 'windows_court', 'windows_street', 'condition', 'building_id','new','district','street',\n",
    "                    'address', 'material', 'elevator_without', 'elevator_passenger', 'elevator_service', 'parking','garbage_chute', 'heating','cluster_number','layout']\n",
    "\n",
    "data_columns = list(data.columns)\n",
    "numerical_data = list(set(data_columns)-set(categorical_data))\n",
    "print(numerical_data)\n",
    "for column in numerical_data:\n",
    "    mean = data[column].mean()\n",
    "    data[column] = data[column].replace(np.NaN, mean)\n",
    "    if column != 'price':\n",
    "        mean_test = data_test[column].mean()\n",
    "        data_test[column] = data_test[column].replace(np.NAN,mean)\n",
    "\n",
    "#Features\n",
    "radius = True\n",
    "district_mean_price = True\n",
    "ohe = True\n",
    "direction = True\n",
    "high_up = True\n",
    "#add high up\n",
    "log_r = False\n",
    "cluster = True\n",
    "\n",
    "\n",
    "\n",
    "if(high_up):\n",
    "    data = add_high_up(data)\n",
    "    data_test = add_high_up(data_test)\n",
    "    data_light = add_high_up(data_light)\n",
    "    data_test_light = add_high_up(data_test_light)\n",
    "\n",
    "data = data.drop(columns = ['area_living', 'area_kitchen'])\n",
    "data_test = data_test.drop(columns = ['area_living', 'area_kitchen'])\n",
    "\n",
    "if(cluster):\n",
    "    data,data_test = cluster_geo_data(data,data_test)\n",
    "    data_light,data_test_light = cluster_geo_data(data_light,data_test_light)\n",
    "\n",
    "if(district_mean_price):\n",
    "    data = group_by_feature_and_price(data,'district')\n",
    "    data_test = group_by_feature_and_price(data_test,'district',data_train=data)\n",
    "    data = data.drop(columns=['price', 'id'])\n",
    "\n",
    "#cat data should not be one hot encoded, and it doest accept nan values\n",
    "data_cat,data_cat_test = data,data_test\n",
    "data_cat = data_cat.fillna('None')\n",
    "data_cat_test = data_cat_test.fillna('None')\n",
    "\n",
    "if(direction):\n",
    "    data = add_direction(data)\n",
    "    data_test = add_direction(data_test)\n",
    "    data = one_hot_encode(data, 'direction')\n",
    "    data_test = one_hot_encode(data_test, 'direction')\n",
    "    data = data.drop(columns = 'direction')\n",
    "    data_test = data_test.drop(columns = 'direction')\n",
    "\n",
    "    data_light = add_direction(data_light)\n",
    "    data_test_light = add_direction(data_test_light)\n",
    "\n",
    "    data_cat = add_direction(data_cat)\n",
    "    data_cat_test = add_direction(data_cat_test)\n",
    "\n",
    "\n",
    "#One hot encode data\n",
    "if(ohe):\n",
    "    encode_categorical_data = ['seller','layout', 'windows_court', 'windows_street', 'condition','new','district',\n",
    "                     'material', 'elevator_without', 'elevator_passenger', 'elevator_service', 'parking','garbage_chute', 'heating','cluster_number']\n",
    "    data = one_hot_encode_multiple(data,encode_categorical_data)\n",
    "    data_test = one_hot_encode_multiple(data_test,encode_categorical_data)\n",
    "\n",
    "if(log_r):\n",
    "    data = log_radius(data)\n",
    "    data_test = log_radius(data_test)\n",
    "    data_light = log_radius(data_light)\n",
    "    data_test_light = log_radius(data_test_light)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Drop cat_data\n",
    "data = data.drop(columns = categorical_data)\n",
    "data = data.drop(columns = ['latitude','longitude'])\n",
    "data_test = data_test.drop(columns=['id','latitude','longitude'])\n",
    "data_test = data_test.drop(columns = categorical_data)\n",
    "print(data.info())\n",
    "\n",
    "focus_light = ['cluster_number','seller','material','district','condition','parking','new','direction','constructed','elevator_without','garbage_chute','loggias','layout','balconies','floor','high_up','rooms','area_total','radius','ceiling','angle']\n",
    "light_features = (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14)\n",
    "focus_cat = ['elevator_passenger', 'elevator_service','windows_court', 'windows_street','cluster_number','seller','material','district','condition','parking','new','direction','constructed','elevator_without','garbage_chute','loggias','layout','balconies','floor','high_up','rooms','area_total','radius','ceiling','angle']\n",
    "categorical_cat = ['elevator_passenger', 'elevator_service','windows_court', 'windows_street','cluster_number','seller','material','district','condition','parking','new','direction','constructed','elevator_without','garbage_chute','loggias','layout','balconies']\n",
    "cat_feature = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "#change cat_data to str for catboost\n",
    "for column in categorical_cat:\n",
    "    data_cat[column]= data_cat[column].astype(str)\n",
    "    data_cat_test[column]= data_cat_test[column].astype(str)\n",
    "\n",
    "#chose which columns to foucs on light gbm and catboost\n",
    "data_train_focus_cat = data_cat[focus_cat]\n",
    "data_test_focus_cat = data_cat_test[focus_cat]\n",
    "\n",
    "data_train_focus_light = data_light[focus_light]\n",
    "data_test_focus_light = data_test_light[focus_light]\n",
    "\n",
    "data_rf, data_test_rf = import_data()\n",
    "\n",
    "data_rf, data_test_rf = rf_data_prep(data_rf,data_test_rf)\n",
    "\n",
    "\n",
    "\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(data_rf, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=42)\n",
    "X_train_light, X_test_light, y_train_light, y_test_light = train_test_split(data_train_focus_light, Y, test_size=0.2, random_state=42)\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(data_train_focus_cat, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('done here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9f943dff1a86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mrf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_rf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mresult_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test_rf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" rf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    388\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    167\u001b[0m                                                         indices=indices)\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \"\"\"\n\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1252\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1253\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    392\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    assert (y_true >= 0).all()\n",
    "    assert (y_pred >= 0).all()\n",
    "    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n",
    "    return np.mean(log_error ** 2) ** 0.5\n",
    "\n",
    "#data.info()\n",
    "#print(len(Y))\n",
    "#print(data.corr())\n",
    "kaggle = True\n",
    "if(kaggle):\n",
    "    Y = np.log(Y)/np.log(15)\n",
    "\n",
    "    rf_model = RandomForestRegressor()\n",
    "    rf_model.fit(data_rf,Y)\n",
    "    result_rf = rf_model.predict(data_test_rf)\n",
    "    print(\" rf\")\n",
    "    \n",
    "    XGB_model = xgb.XGBRegressor(n_estimators = 3000,subsample=0.8,random_state=42)\n",
    "    XGB_model.fit(data,Y)\n",
    "    result_XGB = XGB_model.predict(data_test)\n",
    "    print('1 of 4 done')\n",
    "\n",
    "    XGB_model = xgb.XGBRegressor(n_estimators = 5000,subsample=0.6,random_state=42)\n",
    "    XGB_model.fit(data,Y)\n",
    "    result_XGB_2 = XGB_model.predict(data_test)\n",
    "    print('2 of 4 done')\n",
    "\n",
    "    lightGBM_model = lgb.LGBMRegressor(\n",
    "    categorical_feature=light_features,\n",
    "    num_leaves=9**2,\n",
    "    max_depth=9,\n",
    "    random_state=42,\n",
    "    metric='rmsle',\n",
    "    num_iterations=2000,\n",
    "    learning_rate=0.09,\n",
    "    bagging_freq = 5,\n",
    "    bagging_fraction = 0.8)\n",
    "\n",
    "    lightGBM_model.fit(data_train_focus_light,Y,categorical_feature=light_features)\n",
    "    result_light = lightGBM_model.predict(data_test_focus_light)\n",
    "    print('3 of 4 done')\n",
    "\n",
    "    lightGBM_model = lgb.LGBMRegressor(\n",
    "    categorical_feature=light_features,\n",
    "    num_leaves=7**2,\n",
    "    max_depth=7,\n",
    "    random_state=50,\n",
    "    metric='rmsle',\n",
    "    num_iterations=4000,\n",
    "    learning_rate=0.08,\n",
    "    bagging_freq = 10,\n",
    "    bagging_fraction = 0.6)\n",
    "\n",
    "    lightGBM_model.fit(data_train_focus_light,Y,categorical_feature=light_features)\n",
    "    result_light_2 = lightGBM_model.predict(data_test_focus_light)\n",
    "\n",
    "    prediction_kaggle = (0.1*result_light + 0.3*result_XGB + 0.1*result_light_2 + 0.3*result_XGB_2 + 0.2 *result_rf)\n",
    "\n",
    "    print('done')\n",
    "\n",
    "else:\n",
    "\n",
    "    y_train = np.log(y_train)/np.log(15)\n",
    "    XGB_model = xgb.XGBRegressor(n_estimators = 100,subsample=0.9,random_state=42, max_depth = 8)\n",
    "    XGB_model.fit(X_train,y_train)\n",
    "    prediction_1 = XGB_model.predict(X_test)\n",
    "\n",
    "    XGB_model = xgb.XGBRegressor(n_estimators = 200,eta=0.1, subsample=0.8,random_state=42, max_depth =5)\n",
    "    XGB_model.fit(X_train,y_train)\n",
    "    prediction_2 = XGB_model.predict(X_test)\n",
    "\n",
    "    y_train_light = np.log(y_train_light)/np.log(15)\n",
    "    lightGBM_model = lgb.LGBMRegressor(\n",
    "    categorical_feature=light_features,\n",
    "    num_leaves=9**2,\n",
    "    max_depth=9,\n",
    "    random_state=42,\n",
    "    metric='rmsle',\n",
    "    num_iterations=100,\n",
    "    learning_rate=0.09,\n",
    "    bagging_freq = 5,\n",
    "    bagging_fraction = 0.8)\n",
    "\n",
    "    lightGBM_model.fit(X_train_light,y_train_light,categorical_feature=light_features)\n",
    "    result_light = lightGBM_model.predict(X_test_light)\n",
    "\n",
    "    lightGBM_model = lgb.LGBMRegressor(\n",
    "    categorical_feature=light_features,\n",
    "    num_leaves=7**2,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    metric='rmsle',\n",
    "    num_iterations=200,\n",
    "    learning_rate=0.07,\n",
    "    bagging_freq = 10,\n",
    "    bagging_fraction = 0.7)\n",
    "\n",
    "    lightGBM_model.fit(X_train_light,y_train_light,categorical_feature=light_features)\n",
    "    result_light_2 = lightGBM_model.predict(X_test_light)\n",
    "\n",
    "    y_train_cat_2 = np.log(y_train_cat)/np.log(15)\n",
    "    cat = CatBoostRegressor(loss_function= 'RMSE',verbose=False,random_state = 42, depth=10, learning_rate=0.1, iterations = 200)\n",
    "    cat.fit(X_train_cat,y_train_cat_2,cat_features=cat_feature)\n",
    "    result_cat = cat.predict(X_test_cat)\n",
    "\n",
    "\n",
    "    prediction = (0.15*result_cat + 0.35*prediction_1 + 0.15*result_light + 0.25*prediction_2  + 0.10*result_light_2)\n",
    "    predictions = [result_cat,prediction_1,result_light,prediction_2,result_light_2]\n",
    "    print('done')\n",
    "\n",
    "#lgb.plot_importance(lightGBM_model)\n",
    "#print(data.test)\n",
    "\n",
    "if(kaggle):\n",
    "    prediction_kaggle = 15**prediction_kaggle\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id'] = test_id\n",
    "    submission['price_prediction'] = prediction_kaggle\n",
    "    submission.to_csv('submission_XGB_EDA.csv', index= False)\n",
    "else:\n",
    "    final_prediction = 15**prediction\n",
    "    rmsle = root_mean_squared_log_error(y_test,final_prediction)\n",
    "    print(\"first run\", rmsle)\n",
    "\n",
    "    for i in predictions:\n",
    "        rmsle = root_mean_squared_log_error(y_test,15**i)\n",
    "        print(rmsle)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
