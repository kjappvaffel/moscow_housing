{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "plt.style.use('ggplot')\n",
    "#from feature_engineering import add_retning\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../data')\n",
    "from sklearn import svm\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from display_data import import_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_radius(df):\n",
    "    #adds radius column to dataframe\n",
    "    df['radius'] = np.sqrt((df['latitude']-55.75)**2 + (df['longitude']-37.55)**2)\n",
    "    return df\n",
    "\n",
    "def group_by_feature_and_price(df, feature, data_train = pd.DataFrame()):\n",
    "    #makes a new column with the mean price for each group to all rows\n",
    "    if data_train.empty:\n",
    "        grouped = df.groupby([feature])\n",
    "\n",
    "        mean_price = grouped['price'].mean()\n",
    "\n",
    "        df_merged = pd.merge(df,mean_price, on=feature, how='left')\n",
    "\n",
    "        column_name = str(feature + '_price')\n",
    "\n",
    "        df[column_name] = df_merged['price_y']\n",
    "    else:\n",
    "        grouped = data_train.groupby([feature])\n",
    "\n",
    "        mean_price = grouped['price'].mean()\n",
    "\n",
    "        df_merged = pd.merge(df,mean_price, on=feature, how='left')\n",
    "\n",
    "        column_name = str(feature + '_price')\n",
    "\n",
    "        df[column_name] = df_merged['price']\n",
    "    return df\n",
    "\n",
    "def one_hot_encode(df, column_to_encode):\n",
    "    \"\"\"one hots encode for one singel column\"\"\"\n",
    "    encoded_df = pd.get_dummies(df[[column_to_encode]].astype(str))\n",
    "    df = pd.concat([df,encoded_df],axis=1)\n",
    "    return df\n",
    "\n",
    "def one_hot_encode_multiple(df, list_of_columns):\n",
    "    \"\"\"takes in multiple columns and runs one hot encode for each column\"\"\"\n",
    "    for column_to_encode in list_of_columns:\n",
    "        #print(column_to_encode)\n",
    "        df = one_hot_encode(df, column_to_encode)\n",
    "    return df\n",
    "\n",
    "def add_direction(df):\n",
    "    \"\"\"adds direction to dataframe, can be one of eight categories (N,S,W,E)\"\"\"\n",
    "    #straight up (north)\n",
    "    normal_vector = np.array([0,1])\n",
    "    #normal_vector = np.tile(normal_vector,(df.shape[0],1))\n",
    "    #normal_vector = normal_vector.reshape((2,-df.shape[0]))\n",
    "    temp = pd.DataFrame()\n",
    "    temp['latitude'] = df['latitude']-55.75\n",
    "    temp['longitude'] = df['longitude']-37.56\n",
    "\n",
    "\n",
    "    apartment_vector = temp[['latitude','longitude']].to_numpy()\n",
    "\n",
    "\n",
    "    #print(np.shape(apartment_vector), np.shape(normal_vector))\n",
    "    angles = []\n",
    "    for vector in apartment_vector:\n",
    "        if vector[0] < 0:\n",
    "            temp_angle = -angle_between(vector,normal_vector)\n",
    "        else:\n",
    "            temp_angle = angle_between(vector,normal_vector)\n",
    "        angles.append(temp_angle)\n",
    "\n",
    "    angles = [element * 10 for element in angles]\n",
    "\n",
    "\n",
    "    angles_series = pd.Series(np.array(angles))\n",
    "    #angles_series.plot.hist()\n",
    "\n",
    "    df['direction'] = angles_series\n",
    "    max = df.direction.max().round()\n",
    "    min = df.direction.min().round()\n",
    "    bins = [min,min*7/8,min*5/8,min*3/8,min/8,max/8,max*3/8,max*5/8,max*7/8,max]\n",
    "    rounded_bins = [element.round() for element in bins]\n",
    "    #print(rounded_bins)\n",
    "    direction = pd.cut(df.direction, bins= rounded_bins,labels=['S','SW','W','NW','N','NE','E','SE','S'],ordered=False)\n",
    "    df['direction'] = direction\n",
    "    return df\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\"returns the unit vector if the vector\"\"\"\n",
    "    return vector/np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1,v2):\n",
    "    \"\"\"returns angle between two vectors in radian\"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u,v2_u),-1,1))\n",
    "\n",
    "def fix_radius(data_test):\n",
    "    \"\"\"should just be called on test data\"\"\"\n",
    "    #fix all radius issues\n",
    "    data_test._set_value(23,'radius',0.203899)\n",
    "    data_test._set_value(90,'radius',0.203899)\n",
    "    data_test._set_value(2511,'radius',0.218159)\n",
    "    data_test._set_value(5090,'radius',0.218159)\n",
    "    data_test._set_value(6959,'radius',0.218159)\n",
    "    data_test._set_value(8596,'radius',0.218159)\n",
    "    data_test._set_value(4719, 'radius',0.19580)\n",
    "    data_test._set_value(9547, 'radius',0.19520)\n",
    "    data_test._set_value(2529, 'radius', np.sqrt((37.464994-37.55)**2+(55.627666-55.75)**2))\n",
    "\n",
    "    return data_test\n",
    "\n",
    "def drop_n_largest(data_train):\n",
    "    \"\"\"drops 4 largest values, should only be called on training set\"\"\"\n",
    "    #drop 4 largest from training data, maybe not samrt, but we will see\n",
    "    data_train.drop([3217,21414,15840,13938])\n",
    "    return data_train\n",
    "\n",
    "def clean_data(all_data):\n",
    "    \"\"\"cleans the data with all the knowledge we have so far\"\"\"\n",
    "\n",
    "\n",
    "    #fix ceiling issues\n",
    "    all_data.loc[all_data['ceiling'] > 50,'ceiling']*=0.01\n",
    "    all_data.loc[all_data['ceiling'] > 25, 'ceiling']*=0.1\n",
    "    all_data.loc[all_data['ceiling'] < 0.5,'ceiling'] = float('NaN')\n",
    "    \"\"\"\"\n",
    "    #fix area_kitchen and area_living issues\n",
    "    all_data['living'] = all_data.area_living/all_data.area_total\n",
    "    all_data['kitchen'] = all_data.area_total/all_data.area_kitchen\n",
    "\n",
    "    all_data.loc[all_data['living'] > 1,'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['living'] > 1,'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "\n",
    "    all_data.loc[all_data.area_living/all_data.area_total > 1, 'area_living'] = float('NaN')\n",
    "\n",
    "    all_data['living'] = all_data.area_living/all_data.area_total\n",
    "    all_data['kitchen'] = all_data.area_kitchen/all_data.area_total\n",
    "\n",
    "    all_data['sum_area'] = all_data.area_living + all_data.area_kitchen\n",
    "    all_data.loc[all_data['sum_area'] == 100, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 100, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "\n",
    "    #this fixed some few rows.\n",
    "    #when printing we see some other very clear \"precentage situations, fixing these\n",
    "    all_data.loc[all_data['sum_area'] == 38.5, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 38.5, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 52.7, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 52.7, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 71.6, 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[all_data['sum_area'] == 71.6, 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 20), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 20), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 15), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 15), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 10), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 10), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 30), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 80) & (all_data['area_kitchen'] == 30), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 10), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 10), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 15), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 15), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 20), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 20), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 25), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 25), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 30), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 30), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 50), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 90) & (all_data['area_kitchen'] == 50), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 60) & (all_data['area_total'] > 120), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] == 60) & (all_data['area_total'] > 120), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] < 70) & (all_data['area_total'] > 120)&(all_data['rooms'] < 3), 'area_living'] = all_data['area_living'] * all_data['area_total']/100\n",
    "    all_data.loc[(all_data['sum_area'] < 70) & (all_data['area_total'] > 120)&(all_data['rooms'] < 3), 'area_kitchen'] = all_data['area_kitchen'] * all_data['area_total']/100\n",
    "    \"\"\"\n",
    "    return all_data\n",
    "\n",
    "def add_high_up(df):\n",
    "    \"\"\"add exponetital function to determine how high up a building is\"\"\"\n",
    "    high_up = df.floor/df.stories\n",
    "    high_up_exp = np.exp(high_up) - 1\n",
    "    euler = np.exp(1)\n",
    "\n",
    "    df['high_up'] = high_up_exp\n",
    "    df['high_up'].where(df['high_up'] > euler, euler)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['area_total', 'phones', 'constructed', 'bathrooms_shared', 'latitude', 'ceiling', 'stories', 'price', 'longitude', 'rooms', 'bathrooms_private', 'radius', 'loggias', 'id', 'floor', 'balconies']\n",
      "0         7139520.0\n",
      "1        10500000.0\n",
      "2         9019650.0\n",
      "3        10500000.0\n",
      "4        13900000.0\n",
      "            ...    \n",
      "23280    13300000.0\n",
      "23281    15854300.0\n",
      "23282    19800000.0\n",
      "23283    29999000.0\n",
      "23284    10950000.0\n",
      "Name: price, Length: 23285, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data_SVM, data_test_SVM = import_data()\n",
    "\n",
    "# Missin data 1\n",
    "#imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "#imp.fit(train)\n",
    "#Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)\n",
    "#train_imp = imp.transform(train)\n",
    "\n",
    "Y = data_SVM.price\n",
    "test_id = data_test_SVM.id\n",
    "\n",
    "radius = True\n",
    "# Add radius\n",
    "if(radius):\n",
    "    data_SVM = add_radius(data_SVM)\n",
    "    data_test = add_radius(data_test_SVM)\n",
    "    #coordinates = ['latitude', 'longitude']\n",
    "    #data = data.drop(columns = coordinates)\n",
    "    #data_test = data_test.drop(columns = coordinates)\n",
    "\n",
    "\n",
    "data_SVM = data_SVM.drop(columns = ['area_living', 'area_kitchen'])\n",
    "data_test_SVM = data_test_SVM.drop(columns = ['area_living', 'area_kitchen'])\n",
    "\n",
    "categorical_data = ['seller','layout', 'windows_court', 'windows_street', 'condition', 'building_id','new','district','street',\n",
    "                    'address', 'material', 'elevator_without', 'elevator_passenger', 'elevator_service', 'parking','garbage_chute', 'heating']\n",
    "cleaning = True\n",
    "\n",
    "if(cleaning):\n",
    "    data_test_SVM = fix_radius(data_test_SVM)\n",
    "    data_SVM = clean_data(data_SVM)\n",
    "    data_test_SVM = clean_data(data_test_SVM)\n",
    "\n",
    "data_columns_SVM = list(data_SVM.columns)\n",
    "numerical_data = list(set(data_columns_SVM)-set(categorical_data))\n",
    "print(numerical_data)\n",
    "for column in numerical_data:\n",
    "    mean = data_SVM[column].mean()\n",
    "    data_SVM[column] = data_SVM[column].replace(np.NaN, mean)\n",
    "    if column != 'price':\n",
    "        mean_test = data_test_SVM[column].mean()\n",
    "        data_test_SVM[column] = data_test_SVM[column].replace(np.NAN,mean)\n",
    "\n",
    "#Features\n",
    "radius = True\n",
    "district_mean_price = False #her er det nan\n",
    "ohe = True\n",
    "direction = True\n",
    "high_up = True\n",
    "#add high up\n",
    "\n",
    "\n",
    "if(high_up):\n",
    "    data_SVM = add_high_up(data_SVM)\n",
    "    data_test_SVM = add_high_up(data_test_SVM)\n",
    "\n",
    "if(district_mean_price):\n",
    "    data_SVM = group_by_feature_and_price(data_SVM,'district')\n",
    "    data_test_SVM = group_by_feature_and_price(data_test_SVM,'district',data_train=data_SVM)\n",
    "    data_SVM = data_SVM.drop(columns=['price', 'id'])\n",
    "\n",
    "\n",
    "if(direction):\n",
    "    data_SVM = add_direction(data_SVM)\n",
    "    data_test_SVM = add_direction(data_test_SVM)\n",
    "    data_SVM = one_hot_encode(data_SVM, 'direction')\n",
    "    data_test_SVM = one_hot_encode(data_test_SVM, 'direction')\n",
    "    data_SVM = data_SVM.drop(columns = 'direction')\n",
    "    data_test_SVM = data_test_SVM.drop(columns = 'direction')\n",
    "\n",
    "\n",
    "#One hot encode data\n",
    "if(ohe):\n",
    "    encode_categorical_data_SVM = ['seller','layout', 'windows_court', 'windows_street', 'condition','new','district',\n",
    "                     'material', 'elevator_without', 'elevator_passenger', 'elevator_service', 'parking','garbage_chute', 'heating']\n",
    "    data_SVM = one_hot_encode_multiple(data_SVM,encode_categorical_data_SVM)\n",
    "    data_test_SVM = one_hot_encode_multiple(data_test_SVM,encode_categorical_data_SVM)\n",
    "#####HER MÅ DET FIKSES \n",
    "# skal man logtransformere`?\n",
    "#data_SVM['price'] = np.log(data_SVM['price'])/np.log(15)\n",
    "\n",
    "scaler = MinMaxScaler() # mapper alt til mellom 0 og 1, default\n",
    "data_SVM[numerical_data] = scaler.fit_transform(data_SVM[numerical_data])\n",
    "#Y = scale(Y) ##FIKS her\n",
    "#Y = data_SVM['price']\n",
    "data_SVM = data_SVM.drop(columns=['price', 'id'])\n",
    "\n",
    "#Drop cat_data\n",
    "data_SVM = data_SVM.drop(columns = categorical_data) # har one-hot encoda lengre oppe\n",
    "data_test_SVM = data_test_SVM.drop(columns=['id'])\n",
    "data_test_SVM = data_test_SVM.drop(columns = categorical_data) #må huske testdataen\n",
    " \n",
    "## Rot under her\n",
    "\n",
    "nan_values = data_SVM.isna().any()\n",
    "nan_columns = nan_values.any()\n",
    "\n",
    "\n",
    "columns_with_nan = data_SVM.columns[nan_columns].tolist()\n",
    "#print(nan_values)\n",
    "#print(columns_with_nan)\n",
    "for column in data_SVM.columns:\n",
    "    if data_SVM[column].isna().any():\n",
    "        print(column)\n",
    "\n",
    "print(Y)\n",
    "if (Y.values < 0).any():\n",
    "    print('Gunnar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "4657\n",
      "0\n",
      "first run 0.2690268990590189\n"
     ]
    }
   ],
   "source": [
    "X_train_SVM, X_test_SVM, y_train_SVM, y_test_SVM = train_test_split(data_SVM, Y, test_size=0.2, random_state=42)\n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    assert (y_true >= 0).all()\n",
    "    assert (y_pred >= 0).all()\n",
    "    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n",
    "    return np.mean(log_error ** 2) ** 0.5\n",
    "\n",
    "#data.info()\n",
    "#print(len(Y))\n",
    "#print(data.corr())\n",
    "kaggle = False\n",
    "if(kaggle):\n",
    "    SVM_model = svm.SVR()\n",
    "    Y = np.log(Y)/np.log(15)\n",
    "    SVM_model.fit(data_SVM,Y)\n",
    "    result_SVM = SVM_model.predict(data_test)\n",
    "\n",
    "    result_SVM = 15**result_SVM\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id'] = test_id\n",
    "    submission['price_prediction'] = result_SVM\n",
    "    submission.to_csv('submission_SVM.csv', index= False)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "else:\n",
    "    #y_train = np.log(y_train_SVM)/np.log(15)\n",
    "    SVM_model = svm.SVR()\n",
    "    y_train_SVM = np.log(y_train_SVM)/np.log(15)\n",
    "    SVM_model.fit(X_train_SVM,y_train_SVM)\n",
    "    prediction = SVM_model.predict(X_test_SVM)\n",
    "    prediction = 15**prediction\n",
    "    print(type(prediction))\n",
    "    print(len(prediction))\n",
    "    cunt = 0\n",
    "    meanp = prediction.mean()\n",
    "    for i, p in enumerate(prediction):\n",
    "        if p < 0:\n",
    "            cunt += 1\n",
    "            prediction[i] = meanp\n",
    "    print(cunt)\n",
    "\n",
    "\n",
    "    #prediction = 15**prediction\n",
    "    rmsle = root_mean_squared_log_error(y_test_SVM,prediction)\n",
    "    print(\"first run\", rmsle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forbedringer: \n",
    "Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can be done easily by using a Pipeline:"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
